# üöÄ Gu√≠a: De Notebook a Producci√≥n - Flujo Completo

**Fecha**: 2025-11-01  
**Objetivo**: Gu√≠a paso a paso para que Cristian (o cualquier DS) ponga su modelo en producci√≥n

---

## üìã Requisitos Previos

‚úÖ Contenedor DSDL empresarial funcionando  
‚úÖ Splunk con datos indexados  
‚úÖ Helpers custom instalados  
‚úÖ HEC configurado (telemetr√≠a)

---

## üîÑ Flujo Completo: Dev ‚Üí Prod

### FASE 1: Desarrollo en JupyterLab ‚öôÔ∏è

#### 1.1 Crear Notebook en JupyterLab

**En JupyterLab** (`http://localhost:8888`):

1. Nuevo notebook: `Modelo_Corona_V1.ipynb`
2. Importar helpers y librer√≠as

```python
import sys
sys.path.append("/srv/notebooks_custom/helpers")

from telemetry_helper import log_metrics, log_training_step, log_error
from metrics_calculator import calculate_all_metrics
from preprocessor import standard_preprocessing
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
```

#### 1.2 Definir Funciones Requeridas

**DSDL requiere 3 funciones**:

```python
# 1Ô∏è‚É£ INIT: Inicializaci√≥n del modelo
def init(param):
    """
    Inicializar modelo y par√°metros
    """
    global model, scaler
    
    # Par√°metros del modelo
    epochs = param.get('epochs', 50)
    batch_size = param.get('batch_size', 32)
    learning_rate = param.get('learning_rate', 0.001)
    
    # Definir arquitectura del modelo
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(1, activation='sigmoid')  # Para anomal√≠as
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    # Inicializar preprocessor
    scaler = None
    
    return model

# 2Ô∏è‚É£ FIT: Entrenamiento del modelo
def fit(df, param):
    """
    Entrenar modelo con datos
    """
    global model, scaler
    
    # Separar features y target
    # Asumir que la √∫ltima columna es el target
    feature_cols = [col for col in df.columns if col != 'target']
    X = df[feature_cols].values
    y = df['target'].values if 'target' in df.columns else None
    
    # Preprocesamiento
    X_processed, scaler = standard_preprocessing(X)
    
    # Split train/val
    if y is not None:
        X_train, X_val, y_train, y_val = train_test_split(
            X_processed, y, test_size=0.2, random_state=42
        )
    else:
        # Autoencoder: solo features
        X_train, X_val = train_test_split(X_processed, test_size=0.2, random_state=42)
        y_train, y_val = None, None
    
    # Par√°metros del entrenamiento
    epochs = param.get('epochs', 50)
    batch_size = param.get('batch_size', 32)
    
    # Callbacks
    callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
    ]
    
    # Entrenar
    if y_train is not None:
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=0
        )
    else:
        # Autoencoder: reconstruir input
        history = model.fit(
            X_train, X_train,  # Input = Output
            validation_data=(X_val, X_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=0
        )
    
    # Calcular m√©tricas
    val_pred = model.predict(X_val)
    
    if y_val is not None:
        metrics = calculate_all_metrics(y_val, (val_pred > 0.5).astype(int))
    else:
        # Autoencoder: MSE como m√©trica
        metrics = {
            'mse': np.mean((X_val - val_pred) ** 2),
            'mae': np.mean(np.abs(X_val - val_pred))
        }
    
    # Enviar telemetr√≠a a Splunk
    log_metrics(
        model_name='Modelo_Corona_V1',
        accuracy=metrics.get('accuracy', None),
        f1=metrics.get('f1', None),
        precision=metrics.get('precision', None),
        recall=metrics.get('recall', None)
    )
    
    # Log de entrenamiento
    final_loss = history.history['loss'][-1]
    log_training_step(
        model_name='Modelo_Corona_V1',
        epoch=epochs,
        loss=final_loss
    )
    
    return model

# 3Ô∏è‚É£ APPLY: Aplicar modelo a nuevos datos
def apply(df):
    """
    Aplicar modelo entrenado para predicci√≥n
    """
    global model, scaler
    
    # Separar features
    feature_cols = [col for col in df.columns if col != 'target']
    X = df[feature_cols].values
    
    # Preprocesar con el mismo scaler del entrenamiento
    if scaler is not None:
        X_processed = scaler.transform(X)
    else:
        X_processed = X
    
    # Predecir
    predictions = model.predict(X_processed)
    
    # Si es autoencoder, calcular error de reconstrucci√≥n
    if len(X_processed.shape) > 2 or X_processed.shape[1] == X_processed.shape[0]:
        # Autoencoder: reconstruir y calcular error
        reconstructed = model.predict(X_processed)
        errors = np.mean((X_processed - reconstructed) ** 2, axis=1)
        return errors
    else:
        # Clasificador: probabilidad o clase
        return predictions.flatten()

# 4Ô∏è‚É£ SUMMARY: Resumen del modelo
def summary(df):
    """
    Generar resumen de m√©tricas del modelo
    """
    global model
    
    if model is None:
        return {"status": "Model not initialized"}
    
    return {
        "model_type": "Autoencoder" if y is None else "Classifier",
        "architecture": model.to_json(),
        "trainable_parameters": model.count_params(),
        "layers": len(model.layers)
    }
```

#### 1.3 Guardar Notebook

```python
# Guardar en JupyterLab
# Archivo: Modelo_Corona_V1.ipynb
# Se guarda autom√°ticamente en /srv/notebooks/
```

**DSDL exporta autom√°ticamente** a `/srv/notebooks/app/Modelo_Corona_V1.py`

---

### FASE 2: Testing con Splunk üß™

#### 2.1 Modo Development (mode=stage)

**En Splunk Web ‚Üí Search**:

```spl
index=corona_data 
| head 1000
| fit MLTKContainer algo=Modelo_Corona_V1 mode=stage epochs=10 batch_size=32 
into app:corona_modelo_test
```

**Qu√© hace**:
- ‚úÖ Env√≠a datos al contenedor
- ‚úÖ Abre JupyterLab para debug interactivo
- ‚úÖ **NO guarda modelo** en producci√≥n
- ‚úÖ √ötil para iterar r√°pidamente

#### 2.2 Verificar en JupyterLab

1. Abrir `http://localhost:8888`
2. Ver logs en el notebook
3. Ajustar hiperpar√°metros si es necesario
4. Re-ejecutar `fit` con nuevos par√°metros

---

### FASE 3: Entrenar Modelo Real üéØ

#### 3.1 Fit Command Completo

**En Splunk Web ‚Üí Search**:

```spl
index=corona_data 
| fit MLTKContainer 
    algo=Modelo_Corona_V1 
    epochs=50 
    batch_size=32 
    learning_rate=0.001 
from feature_* 
into app:corona_modelo_v1_produccion
```

**Qu√© hace**:
- ‚úÖ Lee datos de `index=corona_data`
- ‚úÖ Extrae features `feature_*`
- ‚úÖ Entrena modelo con par√°metros especificados
- ‚úÖ Guarda modelo en `app:corona_modelo_v1_produccion`
- ‚úÖ Env√≠a m√©tricas a `ml_metrics`
- ‚úÖ Env√≠a logs a `ml_model_logs`

#### 3.2 Verificar Resultados

**En Splunk Web ‚Üí Search**:

```spl
| rest /servicesNS/-/mltk-container/storage/collections/data/models
| where title="corona_modelo_v1_produccion"
```

O:

```spl
index=ml_metrics model_name=Modelo_Corona_V1
| mstats avg(_value) by metric_name
```

---

### FASE 4: Aplicar Modelo (Inferencia) üîÑ

#### 4.1 Apply Command

**En Splunk Web ‚Üí Search**:

```spl
index=corona_data 
| apply corona_modelo_v1_produccion from feature_*
| eval anomaly_score=_value
| eval is_anomaly=if(anomaly_score > 0.8, 1, 0)
| stats count by is_anomaly
```

**Qu√© hace**:
- ‚úÖ Lee datos de `index=corona_data`
- ‚úÖ Extrae features `feature_*`
- ‚úÖ Aplica modelo entrenado
- ‚úÖ Retorna predicciones en campo `_value`
- ‚úÖ Puedes crear campos derivados (`anomaly_score`, `is_anomaly`)

#### 4.2 Dashboard con Modelo

```spl
index=corona_data
| apply corona_modelo_v1_produccion from feature_*
| eval anomaly_score=_value
| timechart span=1h avg(anomaly_score) as avg_score by sensor
```

---

## üìä Monitoreo y Observabilidad

### Telemetr√≠a Autom√°tica

Los helpers ya configurados env√≠an:

**M√©tricas** (`index=ml_metrics`):
```spl
index=ml_metrics model_name=Modelo_Corona_V1
| mstats avg(_value) by metric_name span=1h
```

**Logs** (`index=ml_model_logs`):
```spl
index=ml_model_logs model_name=Modelo_Corona_V1
| eval hour=strftime(_time, "%H")
| stats count by event_type, hour
```

---

## üîÑ Versionado de Modelos

### Naming Convention

```
{app_name}_{use_case}_{owner}_v{version}

Ejemplos:
- corona_detection_cristian_v1
- corona_detection_cristian_v2
- molino_crudo_cristian_v1
```

### C√≥mo Versionar

1. Entrenar nuevo modelo:
```spl
| fit MLTKContainer algo=Modelo_Corona_V2 ...
into app:corona_detection_cristian_v2
```

2. Comparar modelos:
```spl
index=ml_metrics model_name=Modelo_Corona_V*
| mstats avg(_value) by model_name, metric_name
```

---

## ‚ö†Ô∏è Checklist Pre-Producci√≥n

### Antes de Entrenar

- [ ] Notebook tiene funciones `init`, `fit`, `apply`, `summary`
- [ ] Helpers importados correctamente
- [ ] Telemetr√≠a configurada
- [ ] Datos validados en Splunk
- [ ] Features definidas
- [ ] Hiperpar√°metros decididos

### Despu√©s de Entrenar

- [ ] M√©tricas visibles en `ml_metrics`
- [ ] Logs en `ml_model_logs`
- [ ] Modelo guardado en `app:modelo_*`
- [ ] Accuracy/F1/R¬≤ aceptables
- [ ] Apply funciona sin errores

### Para Producci√≥n

- [ ] Documentar hiperpar√°metros usados
- [ ] Registrar version en Git
- [ ] Crear dashboard de monitoreo
- [ ] Configurar alertas si accuracy baja
- [ ] Plan de retrain peri√≥dico

---

## üìö Ejemplos Completos

### Ejemplo 1: Autoencoder (Anomal√≠as)

Ver notebook: `Molino_Crudo.ipynb` en carpeta Cristian

### Ejemplo 2: Regresi√≥n

Ver notebook: `modelo_gas_corona.ipynb` en carpeta Cristian

### Ejemplo 3: Clasificaci√≥n

Ver notebook: `setpoints_corona_h4.ipynb` en carpeta Cristian

---

## üéØ Pr√≥ximos Pasos para Cristian

1. **Copiar notebook real** de `/Cristian-Autoencoder-Ejemplos/` a JupyterLab
2. **Adaptar funciones** a formato DSDL (`init`, `fit`, `apply`, `summary`)
3. **Agregar telemetr√≠a** con helpers
4. **Probar en modo stage**: `mode=stage`
5. **Entrenar modelo real**: `fit ... into app:modelo_*`
6. **Aplicar modelo**: `apply modelo_*`
7. **Monitorear**: Dashboards con `ml_metrics` y `ml_model_logs`

---

## ‚úÖ Conclusi√≥n

**Flujo Simple**:
1. Crear notebook con `init`, `fit`, `apply`, `summary`
2. Test con `mode=stage`
3. Entrenar con `fit ... into app:modelo_*`
4. Aplicar con `apply modelo_*`
5. Monitorear con dashboards

**Todo autom√°tico** gracias a helpers custom ‚úÖ

**Documentado y testeado** ‚úÖ

