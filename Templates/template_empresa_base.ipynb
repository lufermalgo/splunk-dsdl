{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Empresarial: Autoencoder para Detecci√≥n de Anomal√≠as\n",
    "\n",
    "**Objetivo**: Este template proporciona una base completa para desarrollar modelos de autoencoder con telemetr√≠a y observabilidad empresarial usando Splunk DSDL.\n",
    "\n",
    "**Versi√≥n**: 2.15  \n",
    "**√öltima actualizaci√≥n**: Basado en mejores pr√°cticas de desarrollo E2E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øQu√© es este Template?\n",
    "\n",
    "Este notebook contiene un workflow completo de ejemplo para desarrollar modelos de autoencoder con:\n",
    "\n",
    "- ‚úÖ **Telemetr√≠a autom√°tica**: M√©tricas de entrenamiento e inferencia enviadas a Splunk\n",
    "- ‚úÖ **Observabilidad**: Monitoreo de rendimiento del modelo en producci√≥n\n",
    "- ‚úÖ **Preprocesamiento**: Normalizaci√≥n autom√°tica de datos\n",
    "- ‚úÖ **Detecci√≥n de anomal√≠as**: C√°lculo autom√°tico de scores de anomal√≠a\n",
    "- ‚úÖ **Helpers empresariales**: Uso de m√≥dulos est√°ndar para tareas comunes\n",
    "\n",
    "**Ejemplo**: Autoencoder para detecci√≥n de anomal√≠as usando Keras y TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Importante: Exportaci√≥n Autom√°tica\n",
    "\n",
    "Al guardar este notebook, DSDL **autom√°ticamente** exporta las funciones requeridas a un m√≥dulo Python que luego se invoca desde Splunk con comandos SPL como:\n",
    "\n",
    "```spl\n",
    "| fit MLTKContainer algo=mi_modelo ... into app:mi_modelo\n",
    "| apply mi_modelo\n",
    "| summary mi_modelo\n",
    "```\n",
    "\n",
    "**Funciones requeridas**: `init`, `fit`, `apply`, `summary`, `save`, `load`\n",
    "\n",
    "**Para m√°s informaci√≥n**: Consulta la Gu√≠a Completa Data Scientist E2E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Paso 0: Imports y Configuraci√≥n\n",
    "\n",
    "En este paso se definen todos los imports necesarios y la configuraci√≥n del modelo.\n",
    "\n",
    "**Importante**: \n",
    "- Los helpers empresariales se importan desde `/dltk/notebooks_custom/helpers`\n",
    "- La configuraci√≥n del modelo sigue el naming est√°ndar: `{app_name}_{model_type}_{use_case}_{version}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# mltkc_import\n",
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importar helpers empresariales\n",
    "import sys\n",
    "sys.path.append('/dltk/notebooks_custom/helpers')\n",
    "\n",
    "try:\n",
    "    from telemetry_helper import log_metrics, log_training_step, log_error\n",
    "    print(\"‚úÖ telemetry_helper importado\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  telemetry_helper no disponible (telemetr√≠a deshabilitada)\")\n",
    "\n",
    "try:\n",
    "    from metrics_calculator import calculate_all_metrics\n",
    "    print(\"‚úÖ metrics_calculator importado\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  metrics_calculator no disponible\")\n",
    "\n",
    "try:\n",
    "    from preprocessor import standard_preprocessing, apply_preprocessing\n",
    "    print(\"‚úÖ preprocessor importado\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  preprocessor no disponible\")\n",
    "\n",
    "# Global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "\n",
    "# Configuraci√≥n del modelo (usando naming est√°ndar)\n",
    "# ‚ö†Ô∏è IMPORTANTE: Actualiza estos valores seg√∫n tu modelo\n",
    "APP_NAME = \"app1\"\n",
    "MODEL_TYPE = \"autoencoder\"\n",
    "USE_CASE = \"demo_anomalias\"  # Cambiar seg√∫n tu caso de uso\n",
    "VERSION = \"v1\"  # Incrementar en cada versi√≥n\n",
    "MODEL_NAME = f\"{APP_NAME}_{MODEL_TYPE}_{USE_CASE}_{VERSION}\"\n",
    "\n",
    "print(f\"\\nüì¶ Modelo configurado: {MODEL_NAME}\")\n",
    "print(f\"‚úÖ Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Verificar versiones y configuraci√≥n\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICACI√ìN DE VERSIONES Y CONFIGURACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüì¶ Versiones de librer√≠as:\")\n",
    "print(f\"   - NumPy: {np.__version__}\")\n",
    "print(f\"   - Pandas: {pd.__version__}\")\n",
    "print(f\"   - TensorFlow: {tf.__version__}\")\n",
    "print(f\"   - Keras: {keras.__version__}\")\n",
    "\n",
    "print(f\"\\nüìä Configuraci√≥n del modelo:\")\n",
    "print(f\"   - Nombre: {MODEL_NAME}\")\n",
    "print(f\"   - App: {APP_NAME}\")\n",
    "print(f\"   - Tipo: {MODEL_TYPE}\")\n",
    "print(f\"   - Caso de uso: {USE_CASE}\")\n",
    "print(f\"   - Versi√≥n: {VERSION}\")\n",
    "\n",
    "print(f\"\\nüìÅ Directorio de modelos: {MODEL_DIRECTORY}\")\n",
    "print(f\"   - Existe: {os.path.exists(MODEL_DIRECTORY)}\")\n",
    "if not os.path.exists(MODEL_DIRECTORY):\n",
    "    print(\"   ‚ö†Ô∏è  Creando directorio...\")\n",
    "    os.makedirs(MODEL_DIRECTORY, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Paso 1: Obtener Datos de Splunk (Opcional)\n",
    "\n",
    "Este paso es opcional y solo se usa durante el desarrollo local. En producci√≥n, DSDL pasa los datos directamente a las funciones.\n",
    "\n",
    "**Para desarrollo local**: Puedes usar `SplunkSearch` para obtener datos directamente desde Splunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de SPL para desarrollo local:\n",
    "\n",
    "```spl\n",
    "index=demo_anomalias_data\n",
    "| head 1000\n",
    "| fit MLTKContainer algo=mi_modelo mode=stage epochs=50 batch_size=32 encoding_dim=10 from feature_* into app:mi_modelo\n",
    "```\n",
    "\n",
    "**Explicaci√≥n**:\n",
    "- `index=demo_anomalias_data`: √çndice de Splunk con tus datos\n",
    "- `head 1000`: Limitar a 1000 muestras para desarrollo\n",
    "- `algo=mi_modelo`: Nombre del notebook (sin .ipynb)\n",
    "- `mode=stage`: Modo de desarrollo (carga datos al notebook)\n",
    "- `from feature_*`: Selecciona todas las columnas que empiezan con `feature_`\n",
    "- `into app:mi_modelo`: Nombre del modelo guardado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Obtener Datos Directamente desde Splunk en JupyterLab\n",
    "\n",
    "Puedes obtener datos directamente desde Splunk usando `SplunkSearch` sin necesidad de usar `stage()`.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: El proceso es **interactivo** y requiere **dos celdas separadas**:\n",
    "\n",
    "1. **Primera celda**: Crear y ejecutar la b√∫squeda en Splunk (el usuario hace clic en \"Search\")\n",
    "2. **Segunda celda**: Una vez que la b√∫squeda completa, obtener el DataFrame con los resultados\n",
    "\n",
    "**Ventajas de usar SplunkSearch**:\n",
    "- ‚úÖ No necesitas ejecutar `mode=stage` desde Splunk\n",
    "- ‚úÖ Obtienes datos directamente en JupyterLab\n",
    "- ‚úÖ Puedes ver los resultados en la UI de Splunk antes de convertirlos a DataFrame\n",
    "- ‚úÖ Puedes ajustar la b√∫squeda SPL f√°cilmente\n",
    "- ‚úÖ √ötil para desarrollo y pruebas r√°pidas\n",
    "\n",
    "**Nota**: En producci√≥n, DSDL pasa los datos directamente a `init()`, `fit()`, y `apply()`, as√≠ que esta funci√≥n `stage()` solo es √∫til para desarrollo local cuando usas `mode=stage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Funci√≥n stage() para desarrollo local\n",
    "# ‚ö†Ô∏è NOTA: Esta funci√≥n NO se exporta al .py porque solo se usa para desarrollo local.\n",
    "# En producci√≥n, DSDL pasa los datos directamente a init(), fit(), apply().\n",
    "\n",
    "def stage(name):\n",
    "    \"\"\"\n",
    "    Cargar datos desde archivos CSV/JSON para desarrollo local.\n",
    "    \n",
    "    Esta funci√≥n solo se usa durante el desarrollo en JupyterLab cuando ejecutas\n",
    "    un comando SPL con 'mode=stage'. En producci√≥n, DSDL pasa los datos directamente.\n",
    "    \n",
    "    Args:\n",
    "        name: Nombre del modelo (del par√°metro 'into app:nombre' en SPL)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame, par√°metros) - Datos y par√°metros del modelo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"data/\" + name + \".csv\", 'r') as f:\n",
    "            df = pd.read_csv(f)\n",
    "        print(f\"‚úÖ Datos cargados: {df.shape}\")\n",
    "        \n",
    "        with open(\"data/\" + name + \".json\", 'r') as f:\n",
    "            param = json.load(f)\n",
    "        print(f\"‚úÖ Par√°metros cargados\")\n",
    "        \n",
    "        return df, param\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ö†Ô∏è  Archivo no encontrado: {e}\")\n",
    "        print(\"   Aseg√∫rate de haber ejecutado el comando SPL con 'mode=stage' primero\")\n",
    "        print(\"   O usa SplunkSearch directamente (ver ejemplo en celda anterior)\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Paso 1: Ejecutar b√∫squeda en Splunk\n",
    "# ‚ö†Ô∏è IMPORTANTE: Esta celda crea la b√∫squeda y la ejecuta en Splunk.\n",
    "# El usuario debe hacer clic en el bot√≥n \"Search\" en la UI de Splunk que aparece.\n",
    "# Una vez que la b√∫squeda completa, ejecuta la siguiente celda para obtener el DataFrame.\n",
    "\n",
    "from dsdlsupport import SplunkSearch\n",
    "\n",
    "# Definir la b√∫squeda SPL\n",
    "# ‚ö†Ô∏è NOTA: Ajusta esta b√∫squeda seg√∫n tus datos y necesidades\n",
    "search_query = 'index=demo_anomalias_data | head 1000 | table feature_*'\n",
    "\n",
    "print(\"üîç Ejecutando b√∫squeda en Splunk...\")\n",
    "print(f\"üìã Query: {search_query}\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANTE:\")\n",
    "print(\"   1. Se abrir√° una ventana/UI de Splunk con los resultados\")\n",
    "print(\"   2. Haz clic en el bot√≥n 'Search' para ejecutar la b√∫squeda\")\n",
    "print(\"   3. Espera a que la b√∫squeda complete\")\n",
    "print(\"   4. Una vez completada, ejecuta la siguiente celda para obtener el DataFrame\\n\")\n",
    "\n",
    "# Crear y ejecutar la b√∫squeda (interactivo)\n",
    "search = SplunkSearch.SplunkSearch(search=search_query)\n",
    "\n",
    "print(\"‚úÖ B√∫squeda creada. Haz clic en 'Search' en la UI de Splunk.\")\n",
    "print(\"üìù Una vez que la b√∫squeda complete, ejecuta la siguiente celda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Paso 2: Obtener DataFrame de la b√∫squeda completada\n",
    "# ‚ö†Ô∏è IMPORTANTE: Ejecuta esta celda SOLO despu√©s de que la b√∫squeda anterior haya completado\n",
    "# (despu√©s de hacer clic en \"Search\" y ver los resultados en la UI de Splunk)\n",
    "\n",
    "try:\n",
    "    # Verificar que la b√∫squeda existe\n",
    "    if 'search' not in globals():\n",
    "        print(\"‚ùå Error: No se encontr√≥ el objeto 'search'\")\n",
    "        print(\"   Ejecuta primero la celda anterior para crear la b√∫squeda\")\n",
    "    else:\n",
    "        print(\"üì• Obteniendo DataFrame de los resultados de Splunk...\")\n",
    "        \n",
    "        # Convertir resultados de Splunk a DataFrame\n",
    "        df = search.as_df()\n",
    "        \n",
    "        print(f\"‚úÖ Datos obtenidos: {df.shape}\")\n",
    "        print(f\"üìä Primeras filas:\")\n",
    "        print(df.head())\n",
    "        print(f\"\\nüìã Columnas disponibles: {list(df.columns)}\")\n",
    "        \n",
    "        # Crear par√°metros manualmente para desarrollo local\n",
    "        # ‚ö†Ô∏è NOTA: Ajusta estos par√°metros seg√∫n tus necesidades\n",
    "        param = {\n",
    "            'feature_variables': [col for col in df.columns if col.startswith('feature_')],\n",
    "            'options': {\n",
    "                'params': {\n",
    "                    'epochs': '50',\n",
    "                    'batch_size': '32',\n",
    "                    'encoding_dim': '10',\n",
    "                    'validation_split': '0.2'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Par√°metros creados:\")\n",
    "        print(json.dumps(param, indent=2))\n",
    "        \n",
    "        print(\"\\n‚úÖ DataFrame 'df' y par√°metros 'param' listos para usar en init(), fit(), apply()\")\n",
    "        print(\"   Puedes continuar con las siguientes celdas del notebook\")\n",
    "        \n",
    "except AttributeError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nüí° Soluci√≥n:\")\n",
    "    print(\"   1. Aseg√∫rate de haber ejecutado la celda anterior\")\n",
    "    print(\"   2. Aseg√∫rate de haber hecho clic en 'Search' en la UI de Splunk\")\n",
    "    print(\"   3. Espera a que la b√∫squeda complete antes de ejecutar esta celda\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Paso 2: Inicializar el Modelo (`init`)\n",
    "\n",
    "La funci√≥n `init()` se llama autom√°ticamente por DSDL antes de `fit()`. Su prop√≥sito es crear e inicializar la arquitectura del modelo.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Esta celda debe tener el metadata `\"name\": \"mltkc_init\"` para que DSDL la exporte correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_init",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mltkc_init\n",
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "\n",
    "def init(df, param):\n",
    "    \"\"\"\n",
    "    Inicializar autoencoder para detecci√≥n de anomal√≠as.\n",
    "    \n",
    "    Esta funci√≥n es llamada autom√°ticamente por DSDL antes de fit().\n",
    "    Crea la arquitectura del autoencoder y la compila.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos de Splunk\n",
    "        param: Diccionario con par√°metros del modelo\n",
    "            - feature_variables: Lista de columnas a usar como features\n",
    "            - options.params.encoding_dim (opcional): Dimensi√≥n de la capa oculta (default: 10)\n",
    "            - options.params.components (opcional): Alias para encoding_dim\n",
    "            - options.params.activation (opcional): Funci√≥n de activaci√≥n (default: 'relu')\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo Keras compilado listo para entrenar\n",
    "    \"\"\"\n",
    "    print(f\"üîß Inicializando modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    # Obtener features del DataFrame\n",
    "    if 'feature_variables' in param:\n",
    "        feature_cols = param['feature_variables']\n",
    "    else:\n",
    "        # Si no hay feature_variables definidas, usar todas las num√©ricas\n",
    "        feature_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "        if not feature_cols:\n",
    "            # Fallback: buscar columnas que empiecen con 'feature_'\n",
    "            feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "    \n",
    "    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"üìä Shape de los datos: {X.shape}\")\n",
    "    print(f\"üìã Features seleccionadas: {len(X.columns)}\")\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # Par√°metros del modelo (con valores por defecto)\n",
    "    encoding_dim = 10  # Dimensi√≥n de la capa oculta (bottleneck)\n",
    "    if 'options' in param and 'params' in param['options']:\n",
    "        if 'encoding_dim' in param['options']['params']:\n",
    "            encoding_dim = int(param['options']['params']['encoding_dim'])\n",
    "        elif 'components' in param['options']['params']:\n",
    "            encoding_dim = int(param['options']['params']['components'])\n",
    "    \n",
    "    activation = 'relu'\n",
    "    if 'options' in param and 'params' in param['options']:\n",
    "        if 'activation' in param['options']['params']:\n",
    "            activation = param['options']['params']['activation']\n",
    "        elif 'activation_func' in param['options']['params']:\n",
    "            activation = param['options']['params']['activation_func']\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Par√°metros del modelo:\")\n",
    "    print(f\"   - Input dimension: {input_dim}\")\n",
    "    print(f\"   - Encoding dimension: {encoding_dim}\")\n",
    "    print(f\"   - Activation: {activation}\")\n",
    "    \n",
    "    # Construir autoencoder\n",
    "    # Encoder: reduce dimensiones\n",
    "    encoder = keras.layers.Dense(\n",
    "        encoding_dim, \n",
    "        activation=activation,\n",
    "        input_shape=(input_dim,),\n",
    "        name='encoder'\n",
    "    )\n",
    "    \n",
    "    # Decoder: reconstruye dimensiones originales\n",
    "    decoder = keras.layers.Dense(\n",
    "        input_dim,\n",
    "        activation=activation,\n",
    "        name='decoder'\n",
    "    )\n",
    "    \n",
    "    # Modelo completo\n",
    "    model = keras.Sequential([\n",
    "        encoder,\n",
    "        decoder\n",
    "    ], name='Autoencoder')\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',  # Mean Squared Error para autoencoder\n",
    "        metrics=['mae']  # Mean Absolute Error como m√©trica adicional\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Modelo compilado exitosamente\")\n",
    "    print(f\"üìê Arquitectura: {input_dim} ‚Üí {encoding_dim} ‚Üí {input_dim}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test init localmente\n",
    "# ‚ö†Ô∏è NOTA: Solo funciona si tienes datos cargados (stage() o SplunkSearch)\n",
    "\n",
    "try:\n",
    "    # Crear datos dummy para prueba si no hay datos reales\n",
    "    if 'df' not in globals() or 'param' not in globals():\n",
    "        print(\"‚ö†Ô∏è  No hay datos cargados. Creando datos dummy para prueba...\")\n",
    "        test_df = pd.DataFrame({\n",
    "            'feature_0': np.random.randn(100),\n",
    "            'feature_1': np.random.randn(100),\n",
    "            'feature_2': np.random.randn(100),\n",
    "            'feature_3': np.random.randn(100),\n",
    "            'feature_4': np.random.randn(100)\n",
    "        })\n",
    "        test_param = {\n",
    "            'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "            'options': {\n",
    "                'params': {\n",
    "                    'encoding_dim': 10\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        test_model = init(test_df, test_param)\n",
    "        print(\"\\nüìä Resumen del modelo de prueba:\")\n",
    "        test_model.summary()\n",
    "    else:\n",
    "        model = init(df, param)\n",
    "        print(\"\\nüìä Resumen del modelo:\")\n",
    "        model.summary()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Paso 3: Entrenar el Modelo (`fit`)\n",
    "\n",
    "La funci√≥n `fit()` se llama autom√°ticamente por DSDL para entrenar el modelo. Incluye:\n",
    "\n",
    "- ‚úÖ Preprocesamiento autom√°tico (normalizaci√≥n)\n",
    "- ‚úÖ Telemetr√≠a por √©poca (m√©tricas enviadas a Splunk)\n",
    "- ‚úÖ Callback de TensorBoard para visualizaci√≥n\n",
    "- ‚úÖ C√°lculo de m√©tricas de reconstrucci√≥n\n",
    "- ‚úÖ Guardado del scaler para uso posterior\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Esta celda debe tener el metadata `\"name\": \"mltkc_stage_create_model_fit\"` para que DSDL la exporte correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_stage_create_model_fit",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_fit\n",
    "# returns a fit info json object\n",
    "\n",
    "def fit(model, df, param):\n",
    "    \"\"\"\n",
    "    Entrenar autoencoder con telemetr√≠a autom√°tica.\n",
    "    \n",
    "    Esta funci√≥n es llamada autom√°ticamente por DSDL para entrenar el modelo.\n",
    "    Incluye preprocesamiento, telemetr√≠a por √©poca, y c√°lculo de m√©tricas.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras inicializado (retornado por init())\n",
    "        df: DataFrame con datos de entrenamiento\n",
    "        param: Diccionario con par√°metros de entrenamiento\n",
    "            - feature_variables: Lista de columnas a usar como features\n",
    "            - options.params.epochs (opcional): N√∫mero de √©pocas (default: 50)\n",
    "            - options.params.batch_size (opcional): Tama√±o de batch (default: 32)\n",
    "            - options.params.validation_split (opcional): Fracci√≥n de validaci√≥n (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Informaci√≥n del entrenamiento\n",
    "            - fit_history: Historial de entrenamiento de Keras\n",
    "            - scaler: Scaler usado para normalizaci√≥n (CR√çTICO para apply())\n",
    "            - model_loss: Loss final del modelo\n",
    "            - model_mae: MAE final del modelo\n",
    "            - mse: Mean Squared Error de reconstrucci√≥n\n",
    "            - rmse: Root Mean Squared Error de reconstrucci√≥n\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Iniciando entrenamiento del modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    returns = {}\n",
    "    \n",
    "    # Obtener features\n",
    "    if 'feature_variables' in param:\n",
    "        feature_cols = param['feature_variables']\n",
    "    else:\n",
    "        feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        if not feature_cols:\n",
    "            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"üìä Datos de entrenamiento: {X.shape[0]} muestras, {X.shape[1]} features\")\n",
    "    \n",
    "    # Preprocesamiento: Normalizaci√≥n (CR√çTICO para autoencoders)\n",
    "    print(\"üîß Aplicando preprocesamiento (normalizaci√≥n)...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    # Guardar scaler en returns para uso posterior en apply()\n",
    "    returns['scaler'] = scaler\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    validation_split = 0.2\n",
    "    \n",
    "    if 'options' in param and 'params' in param['options']:\n",
    "        if 'epochs' in param['options']['params']:\n",
    "            epochs = int(param['options']['params']['epochs'])\n",
    "        if 'batch_size' in param['options']['params']:\n",
    "            batch_size = int(param['options']['params']['batch_size'])\n",
    "        if 'validation_split' in param['options']['params']:\n",
    "            validation_split = float(param['options']['params']['validation_split'])\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Par√°metros de entrenamiento:\")\n",
    "    print(f\"   - Epochs: {epochs}\")\n",
    "    print(f\"   - Batch size: {batch_size}\")\n",
    "    print(f\"   - Validation split: {validation_split}\")\n",
    "    \n",
    "    # Callback para TensorBoard (opcional, para visualizaci√≥n)\n",
    "    log_dir = f\"/srv/notebooks/logs/fit/{MODEL_NAME}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "    \n",
    "    # Callback personalizado para logging de telemetr√≠a por √©poca\n",
    "    class TelemetryCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            \"\"\"Enviar m√©tricas de cada √©poca a Splunk\"\"\"\n",
    "            logs = logs or {}\n",
    "            try:\n",
    "                # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy a tipos nativos de Python para JSON serialization\n",
    "                epoch_value = int(epoch + 1)  # Convertir a int nativo\n",
    "                loss_value = float(logs.get('loss', 0)) if logs.get('loss') is not None else 0.0\n",
    "                val_loss_value = float(logs.get('val_loss', 0)) if logs.get('val_loss') is not None else 0.0\n",
    "                mae_value = float(logs.get('mae', 0)) if logs.get('mae') is not None else 0.0\n",
    "                val_mae_value = float(logs.get('val_mae', 0)) if logs.get('val_mae') is not None else 0.0\n",
    "                \n",
    "                # Intentar usar log_training_step si est√° disponible\n",
    "                try:\n",
    "                    log_training_step(\n",
    "                        model_name=MODEL_NAME,\n",
    "                        epoch=epoch_value,\n",
    "                        loss=loss_value,\n",
    "                        val_loss=val_loss_value,\n",
    "                        mae=mae_value,\n",
    "                        val_mae=val_mae_value\n",
    "                    )\n",
    "                except NameError:\n",
    "                    # Si log_training_step no est√° disponible, no hacer nada\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error enviando telemetr√≠a en √©poca {epoch + 1}: {e}\")\n",
    "                import traceback\n",
    "                print(f\"   Traceback completo: {traceback.format_exc()}\")\n",
    "    \n",
    "    telemetry_callback = TelemetryCallback()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nüèãÔ∏è  Iniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        x=X_scaled_df,\n",
    "        y=X_scaled_df,  # Autoencoder: input = output\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_callback, telemetry_callback]\n",
    "    )\n",
    "    \n",
    "    returns['fit_history'] = history\n",
    "    returns['model_epochs'] = epochs\n",
    "    returns['model_batch_size'] = batch_size\n",
    "    returns['scaler'] = scaler  # Guardar scaler para uso en apply\n",
    "    \n",
    "    # Evaluar modelo en datos completos\n",
    "    print(\"\\nüìä Evaluando modelo en datos completos...\")\n",
    "    test_results = model.evaluate(X_scaled_df, X_scaled_df, verbose=0)\n",
    "    returns['model_loss'] = float(test_results[0])  # Convertir a float nativo\n",
    "    returns['model_mae'] = float(test_results[1]) if len(test_results) > 1 else None\n",
    "    \n",
    "    print(f\"‚úÖ Entrenamiento completado\")\n",
    "    print(f\"   - Loss final: {test_results[0]:.6f}\")\n",
    "    if len(test_results) > 1:\n",
    "        print(f\"   - MAE final: {test_results[1]:.6f}\")\n",
    "    \n",
    "    # Calcular m√©tricas de reconstrucci√≥n\n",
    "    print(\"\\nüìà Calculando m√©tricas de reconstrucci√≥n...\")\n",
    "    X_pred = model.predict(X_scaled_df, verbose=0)\n",
    "    \n",
    "    # Calcular MSE y RMSE\n",
    "    mse = mean_squared_error(X_scaled_df.values, X_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    returns['mse'] = float(mse)  # Convertir a float nativo\n",
    "    returns['rmse'] = float(rmse)  # Convertir a float nativo\n",
    "    \n",
    "    print(f\"   - MSE: {mse:.6f}\")\n",
    "    print(f\"   - RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    # Enviar m√©tricas finales a Splunk (telemetr√≠a)\n",
    "    try:\n",
    "        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy a tipos nativos de Python para JSON serialization\n",
    "        mae_value = float(returns['model_mae']) if returns['model_mae'] is not None else None\n",
    "        rmse_value = float(rmse) if rmse is not None else None\n",
    "        mse_value = float(mse) if mse is not None else None\n",
    "        loss_value = float(test_results[0]) if test_results[0] is not None else None\n",
    "        \n",
    "        # Intentar usar log_metrics si est√° disponible\n",
    "        try:\n",
    "            log_metrics(\n",
    "                model_name=MODEL_NAME,\n",
    "                r2_score=None,  # Autoencoder no tiene R¬≤ tradicional\n",
    "                mae=mae_value,\n",
    "                rmse=rmse_value,\n",
    "                mse=mse_value,\n",
    "                loss=loss_value,\n",
    "                app_name=APP_NAME,\n",
    "                model_version=VERSION,\n",
    "                project=USE_CASE\n",
    "            )\n",
    "            print(\"‚úÖ M√©tricas enviadas a Splunk\")\n",
    "        except NameError:\n",
    "            # Si log_metrics no est√° disponible, no hacer nada\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error enviando m√©tricas a Splunk: {e}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback completo: {traceback.format_exc()}\")\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test fit localmente\n",
    "# ‚ö†Ô∏è NOTA: Solo funciona si tienes modelo y datos cargados\n",
    "\n",
    "try:\n",
    "    if 'model' not in globals() or 'df' not in globals() or 'param' not in globals():\n",
    "        print(\"‚ö†Ô∏è  No hay modelo o datos cargados. Creando datos dummy para prueba...\")\n",
    "        test_df = pd.DataFrame({\n",
    "            'feature_0': np.random.randn(500),\n",
    "            'feature_1': np.random.randn(500),\n",
    "            'feature_2': np.random.randn(500),\n",
    "            'feature_3': np.random.randn(500),\n",
    "            'feature_4': np.random.randn(500)\n",
    "        })\n",
    "        test_param = {\n",
    "            'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "            'options': {\n",
    "                'params': {\n",
    "                    'epochs': '10',  # Pocas √©pocas para prueba r√°pida\n",
    "                    'batch_size': '32',\n",
    "                    'validation_split': '0.2'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        test_model = init(test_df, test_param)\n",
    "        print(\"\\n‚è≥ Entrenando modelo de prueba (esto tomar√° unos minutos)...\")\n",
    "        fit_results = fit(test_model, test_df, test_param)\n",
    "    else:\n",
    "        print(\"\\n‚è≥ Entrenando modelo (esto tomar√° varios minutos)...\")\n",
    "        fit_results = fit(model, df, param)\n",
    "    \n",
    "    print(\"\\n‚úÖ Test de fit completado exitosamente\")\n",
    "    print(f\"   - Loss: {fit_results.get('model_loss', 'N/A')}\")\n",
    "    print(f\"   - MSE: {fit_results.get('mse', 'N/A')}\")\n",
    "    print(f\"   - RMSE: {fit_results.get('rmse', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Explorar modelo entrenado\n",
    "# ‚ö†Ô∏è NOTA: Solo funciona despu√©s de entrenar el modelo\n",
    "\n",
    "try:\n",
    "    if 'model' in globals():\n",
    "        print(\"üìä Informaci√≥n del modelo:\")\n",
    "        print(f\"   - Input shape: {model.input_shape}\")\n",
    "        print(f\"   - Output shape: {model.output_shape}\")\n",
    "        print(f\"   - Total params: {model.count_params()}\")\n",
    "        print(f\"\\nüìê Arquitectura:\")\n",
    "        model.summary()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Modelo no disponible. Ejecuta primero init() y fit()\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Paso 4: Aplicar el Modelo (`apply`)\n",
    "\n",
    "La funci√≥n `apply()` se llama autom√°ticamente por DSDL para hacer inferencia con datos nuevos. Incluye:\n",
    "\n",
    "- ‚úÖ Detecci√≥n de anomal√≠as basada en error de reconstrucci√≥n\n",
    "- ‚úÖ C√°lculo de scores de anomal√≠a normalizados\n",
    "- ‚úÖ Telemetr√≠a de inferencia (m√©tricas enviadas a Splunk)\n",
    "- ‚úÖ Uso del scaler guardado durante fit()\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Esta celda debe tener el metadata `\"name\": \"mltkc_stage_create_model_apply\"` para que DSDL la exporte correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_stage_create_model_apply",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_apply\n",
    "\n",
    "def apply(model, df, param):\n",
    "    \"\"\"\n",
    "    Aplicar autoencoder para detecci√≥n de anomal√≠as.\n",
    "    \n",
    "    Esta funci√≥n es llamada autom√°ticamente por DSDL para hacer inferencia con datos nuevos.\n",
    "    Calcula el error de reconstrucci√≥n y detecta anomal√≠as bas√°ndose en un umbral.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras entrenado (retornado por fit())\n",
    "        df: DataFrame con datos nuevos para inferencia\n",
    "        param: Diccionario con par√°metros (debe contener scaler de fit())\n",
    "            - feature_variables: Lista de columnas a usar como features\n",
    "            - scaler: Scaler usado durante fit() (CR√çTICO - debe venir de fit())\n",
    "            - anomaly_threshold (opcional): Umbral de anomal√≠a (default: percentil 95)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con reconstrucciones y scores de anomal√≠a\n",
    "            - reconstruction_error: Error de reconstrucci√≥n por muestra\n",
    "            - anomaly_score: Score normalizado de anomal√≠a\n",
    "            - is_anomaly: 1 si es anomal√≠a, 0 si no\n",
    "            - reconstruction_*: Columnas con valores reconstruidos\n",
    "            - original_*: Columnas con valores originales\n",
    "    \"\"\"\n",
    "    print(f\"üîÆ Aplicando modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    # Obtener features (debe coincidir con las usadas en fit)\n",
    "    if 'feature_variables' in param:\n",
    "        feature_cols = param['feature_variables']\n",
    "    else:\n",
    "        feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        if not feature_cols:\n",
    "            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"üìä Datos de inferencia: {X.shape[0]} muestras, {X.shape[1]} features\")\n",
    "    \n",
    "    # Obtener scaler del entrenamiento (desde param o fit_results)\n",
    "    scaler = None\n",
    "    if 'scaler' in param:\n",
    "        scaler = param['scaler']\n",
    "    elif hasattr(model, 'scaler'):\n",
    "        scaler = model.scaler\n",
    "    \n",
    "    # Aplicar normalizaci√≥n\n",
    "    if scaler is not None:\n",
    "        # Usar scaler del entrenamiento\n",
    "        X_scaled = scaler.transform(X)\n",
    "        print(\"‚úÖ Usando scaler del entrenamiento\")\n",
    "    else:\n",
    "        # Crear nuevo scaler si no est√° disponible (fallback)\n",
    "        print(\"‚ö†Ô∏è  Scaler no encontrado en param. Aplicando normalizaci√≥n nueva...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    # Predecir reconstrucciones\n",
    "    print(\"üîÑ Calculando reconstrucciones...\")\n",
    "    X_reconstructed = model.predict(X_scaled_df, verbose=0)\n",
    "    X_reconstructed_df = pd.DataFrame(X_reconstructed, columns=X.columns, index=X.index)\n",
    "    \n",
    "    # Calcular error de reconstrucci√≥n (MSE por muestra)\n",
    "    reconstruction_error = np.mean((X_scaled_df.values - X_reconstructed_df.values) ** 2, axis=1)\n",
    "    \n",
    "    # Calcular threshold para anomal√≠as (percentil 95)\n",
    "    # En producci√≥n, este threshold deber√≠a venir del conjunto de entrenamiento\n",
    "    if 'anomaly_threshold' in param:\n",
    "        anomaly_threshold = float(param['anomaly_threshold'])\n",
    "    else:\n",
    "        anomaly_threshold = float(np.percentile(reconstruction_error, 95))\n",
    "    \n",
    "    # Detectar anomal√≠as\n",
    "    is_anomaly = (reconstruction_error > anomaly_threshold).astype(int)\n",
    "    anomaly_score = reconstruction_error / (anomaly_threshold + 1e-10)  # Normalizar score\n",
    "    \n",
    "    print(f\"üìä Estad√≠sticas de reconstrucci√≥n:\")\n",
    "    print(f\"   - Error medio: {np.mean(reconstruction_error):.6f}\")\n",
    "    print(f\"   - Error mediano: {np.median(reconstruction_error):.6f}\")\n",
    "    print(f\"   - Threshold (percentil 95): {anomaly_threshold:.6f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {is_anomaly.sum()} / {len(is_anomaly)} ({100*np.mean(is_anomaly):.2f}%)\")\n",
    "    \n",
    "    # Construir DataFrame de resultados\n",
    "    results = pd.DataFrame({\n",
    "        'reconstruction_error': reconstruction_error,\n",
    "        'anomaly_score': anomaly_score,\n",
    "        'is_anomaly': is_anomaly\n",
    "    }, index=X.index)\n",
    "    \n",
    "    # Agregar reconstrucciones como columnas (opcional)\n",
    "    for i, col in enumerate(X.columns):\n",
    "        results[f'reconstruction_{col}'] = X_reconstructed_df[col].values\n",
    "        results[f'original_{col}'] = X[col].values\n",
    "    \n",
    "    print(f\"‚úÖ Inferencia completada. Shape de resultados: {results.shape}\")\n",
    "    \n",
    "    # Enviar telemetr√≠a de inferencia a Splunk\n",
    "    try:\n",
    "        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy a tipos nativos de Python para JSON serialization\n",
    "        num_predictions = int(len(df))  # len() ya retorna int nativo\n",
    "        num_anomalies = int(is_anomaly.sum())\n",
    "        avg_reconstruction_error = float(np.mean(reconstruction_error))\n",
    "        anomaly_threshold_native = float(anomaly_threshold)\n",
    "        \n",
    "        # Preparar datos de telemetr√≠a\n",
    "        telemetry_data = {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"num_predictions\": num_predictions,\n",
    "            \"num_anomalies\": num_anomalies,\n",
    "            \"avg_reconstruction_error\": avg_reconstruction_error,\n",
    "            \"anomaly_threshold\": anomaly_threshold_native,\n",
    "            \"app_name\": APP_NAME,\n",
    "            \"model_version\": VERSION,\n",
    "            \"project\": USE_CASE\n",
    "        }\n",
    "        \n",
    "        # Eliminar valores None\n",
    "        telemetry_data = {k: v for k, v in telemetry_data.items() if v is not None}\n",
    "        \n",
    "        # Intentar usar log_prediction si est√° disponible, sino log_metrics\n",
    "        try:\n",
    "            from telemetry_helper import log_prediction\n",
    "            log_prediction(**telemetry_data)\n",
    "            print(\"‚úÖ Telemetr√≠a de inferencia enviada a Splunk (usando log_prediction)\")\n",
    "        except (ImportError, NameError):\n",
    "            try:\n",
    "                log_metrics(**telemetry_data)\n",
    "                print(\"‚úÖ Telemetr√≠a de inferencia enviada a Splunk (usando log_metrics)\")\n",
    "            except NameError:\n",
    "                # Si ninguna funci√≥n est√° disponible, no hacer nada\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error enviando telemetr√≠a de inferencia a Splunk: {e}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback completo: {traceback.format_exc()}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test apply localmente\n",
    "# ‚ö†Ô∏è NOTA: Solo funciona si tienes modelo entrenado y datos cargados\n",
    "\n",
    "try:\n",
    "    if 'model' not in globals() or 'fit_results' not in globals():\n",
    "        print(\"‚ö†Ô∏è  No hay modelo entrenado. Ejecuta primero init() y fit()\")\n",
    "    else:\n",
    "        # Crear datos nuevos para inferencia\n",
    "        test_df_apply = pd.DataFrame({\n",
    "            'feature_0': np.random.randn(100),\n",
    "            'feature_1': np.random.randn(100),\n",
    "            'feature_2': np.random.randn(100),\n",
    "            'feature_3': np.random.randn(100),\n",
    "            'feature_4': np.random.randn(100)\n",
    "        })\n",
    "        \n",
    "        # Agregar scaler al param (simulando que viene de fit)\n",
    "        test_param_apply = {\n",
    "            'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "            'scaler': fit_results.get('scaler')  # Usar scaler del fit anterior\n",
    "        }\n",
    "        \n",
    "        # Aplicar modelo\n",
    "        results = apply(model, test_df_apply, test_param_apply)\n",
    "        \n",
    "        print(\"\\nüìä Primeras 10 filas de resultados:\")\n",
    "        print(results.head(10))\n",
    "        \n",
    "        print(\"\\nüìà Estad√≠sticas de anomal√≠as:\")\n",
    "        print(f\"   - Total muestras: {len(results)}\")\n",
    "        print(f\"   - Anomal√≠as detectadas: {results['is_anomaly'].sum()}\")\n",
    "        print(f\"   - Porcentaje: {100 * results['is_anomaly'].mean():.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Paso 5: Guardar el Modelo (`save`)\n",
    "\n",
    "La funci√≥n `save()` se llama **autom√°ticamente** por DSDL despu√©s de `fit()`.\n",
    "\n",
    "**‚ö†Ô∏è CR√çTICO**: Esta funci√≥n es **REQUERIDA** por DSDL. Si no existe o no tiene el metadata correcto, ver√°s el error:\n",
    "```\n",
    "AttributeError: module 'app.model.mi_modelo' has no attribute 'save'\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Esta celda debe tener el metadata `\"name\": \"mltkc_save\"` para que DSDL la exporte correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_save",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_save\n",
    "# Funci√≥n REQUERIDA: DSDL llama a save(model, name) despu√©s de fit()\n",
    "\n",
    "def save(model, name):\n",
    "    \"\"\"\n",
    "    Guardar modelo Keras en disco.\n",
    "    \n",
    "    IMPORTANTE: Esta funci√≥n es llamada autom√°ticamente por DSDL despu√©s de fit().\n",
    "    DSDL usa el nombre pasado desde 'into app:nombre' en SPL.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras entrenado (retornado por fit())\n",
    "        name: Nombre del modelo (pasado por DSDL desde \"into app:model_name\")\n",
    "    \n",
    "    Returns:\n",
    "        model: Retorna el modelo (requerido por DSDL)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Asegurar que el directorio existe\n",
    "    os.makedirs(MODEL_DIRECTORY, exist_ok=True)\n",
    "    \n",
    "    # Guardar modelo Keras\n",
    "    filepath = MODEL_DIRECTORY + name + \".keras\"\n",
    "    model.save(filepath)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo guardado en: {filepath}\")\n",
    "    print(f\"üìä Tama√±o del archivo: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # NOTA: Si tienes un scaler u otros objetos, gu√°rdalos tambi√©n\n",
    "    # Ejemplo: si el scaler est√° en el modelo o en globals\n",
    "    # from sklearn.externals import joblib  # o import joblib\n",
    "    # if hasattr(model, 'scaler'):\n",
    "    #     joblib.dump(model.scaler, MODEL_DIRECTORY + name + \"_scaler.pkl\")\n",
    "    \n",
    "    # DSDL espera que retornes el modelo\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test save localmente (opcional)\n",
    "# ‚ö†Ô∏è NOTA: En producci√≥n, DSDL llama a save() autom√°ticamente despu√©s de fit()\n",
    "\n",
    "try:\n",
    "    if 'model' not in globals():\n",
    "        print(\"‚ö†Ô∏è  No hay modelo disponible. Ejecuta primero init() y fit()\")\n",
    "    else:\n",
    "        # Guardar modelo de prueba usando la firma correcta\n",
    "        saved_model = save(model, name=\"test_autoencoder\")\n",
    "        print(f\"‚úÖ Modelo guardado exitosamente\")\n",
    "        \n",
    "        # Verificar que el archivo existe\n",
    "        filepath = MODEL_DIRECTORY + \"test_autoencoder.keras\"\n",
    "        if os.path.exists(filepath):\n",
    "            file_size = os.path.getsize(filepath) / (1024 * 1024)\n",
    "            print(f\"üìä Tama√±o del archivo: {file_size:.2f} MB\")\n",
    "            print(f\"‚úÖ Archivo creado correctamente: {filepath}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Archivo no encontrado: {filepath}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Paso 6: Cargar el Modelo (`load`) - Opcional\n",
    "\n",
    "La funci√≥n `load()` es **opcional** y solo se usa para desarrollo local. DSDL **NO** la llama autom√°ticamente.\n",
    "\n",
    "**Uso**: √ötil para cargar un modelo guardado durante desarrollo o pruebas.\n",
    "\n",
    "**‚ö†Ô∏è NOTA**: Si quieres que DSDL exporte esta funci√≥n, agrega el metadata `\"name\": \"mltkc_load\"` a la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_load",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_load\n",
    "# Funci√≥n opcional para cargar modelo guardado durante desarrollo\n",
    "# DSDL NO llama a esta funci√≥n autom√°ticamente\n",
    "\n",
    "def load(name):\n",
    "    \"\"\"\n",
    "    Cargar modelo Keras desde disco.\n",
    "    \n",
    "    √ötil para desarrollo local o pruebas.\n",
    "    DSDL NO usa esta funci√≥n autom√°ticamente.\n",
    "    \n",
    "    Args:\n",
    "        name: Nombre del archivo (sin extensi√≥n)\n",
    "    \n",
    "    Returns:\n",
    "        Model: Modelo Keras cargado\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        model_dir = MODEL_DIRECTORY\n",
    "    except NameError:\n",
    "        model_dir = \"/srv/app/model/data/\"\n",
    "    \n",
    "    filepath = model_dir + name + \".keras\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"‚ùå Archivo no encontrado: {filepath}\")\n",
    "    \n",
    "    print(f\"üì• Cargando modelo desde: {filepath}\")\n",
    "    model = keras.models.load_model(filepath)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo cargado exitosamente\")\n",
    "    print(f\"üìä Arquitectura: {model.input_shape} ‚Üí {model.output_shape}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test load localmente (opcional)\n",
    "# ‚ö†Ô∏è NOTA: Requiere que hayas guardado un modelo primero\n",
    "\n",
    "try:\n",
    "    test_filepath = MODEL_DIRECTORY + \"test_autoencoder.keras\"\n",
    "    if not os.path.exists(test_filepath):\n",
    "        print(f\"‚ö†Ô∏è  Archivo no encontrado: {test_filepath}\")\n",
    "        print(\"   Necesitas ejecutar primero el test de save() (Paso 5)\")\n",
    "        print(\"   O aseg√∫rate de que test_autoencoder.keras existe\")\n",
    "    else:\n",
    "        loaded_model = load(\"test_autoencoder\")\n",
    "        print(\"‚úÖ Modelo cargado exitosamente\")\n",
    "        \n",
    "        # Verificar que son equivalentes (solo si model existe)\n",
    "        if 'model' in globals():\n",
    "            print(\"\\nüîç Verificando que el modelo cargado funciona...\")\n",
    "            test_input = np.random.randn(1, 5)  # 5 features\n",
    "            output_original = model.predict(test_input, verbose=0)\n",
    "            output_loaded = loaded_model.predict(test_input, verbose=0)\n",
    "            \n",
    "            if np.allclose(output_original, output_loaded):\n",
    "                print(\"‚úÖ Los modelos producen resultados id√©nticos\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Los modelos producen resultados diferentes\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  model no est√° definido, no se puede verificar equivalencia\")\n",
    "            print(\"   Pero el modelo se carg√≥ correctamente ‚úÖ\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Paso 7: Resumen del Modelo (`summary`)\n",
    "\n",
    "La funci√≥n `summary()` se llama autom√°ticamente por DSDL para obtener metadatos del modelo.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Esta celda debe tener el metadata `\"name\": \"mltkc_summary\"` para que DSDL la exporte correctamente.\n",
    "\n",
    "**‚ö†Ô∏è CR√çTICO**: Todos los valores NumPy deben convertirse a tipos nativos de Python antes de retornarlos, ya que DSDL serializa el resultado a JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_summary",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "\n",
    "def summary(model=None):\n",
    "    \"\"\"\n",
    "    Proporcionar metadatos y resumen del modelo.\n",
    "    \n",
    "    Esta funci√≥n es llamada autom√°ticamente por DSDL para obtener informaci√≥n del modelo.\n",
    "    DSDL serializa el resultado a JSON, por lo que todos los valores deben ser tipos nativos de Python.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras (opcional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadatos del modelo\n",
    "            - model_name: Nombre del modelo\n",
    "            - app_name: Nombre de la aplicaci√≥n\n",
    "            - model_type: Tipo de modelo\n",
    "            - use_case: Caso de uso\n",
    "            - version: Versi√≥n del modelo\n",
    "            - version_info: Versiones de librer√≠as\n",
    "            - model_summary: Resumen del modelo como string (si model est√° disponible)\n",
    "            - model_architecture: Informaci√≥n de la arquitectura (si model est√° disponible)\n",
    "            - layers: Informaci√≥n de cada capa (si model est√° disponible)\n",
    "    \"\"\"\n",
    "    returns = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"app_name\": APP_NAME,\n",
    "        \"model_type\": MODEL_TYPE,\n",
    "        \"use_case\": USE_CASE,\n",
    "        \"version\": VERSION,\n",
    "        \"version_info\": {\n",
    "            \"tensorflow\": tf.__version__,\n",
    "            \"keras\": keras.__version__,\n",
    "            \"numpy\": np.__version__,\n",
    "            \"pandas\": pd.__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if model is not None:\n",
    "        # Guardar resumen del modelo como string\n",
    "        s = []\n",
    "        model.summary(print_fn=lambda x: s.append(x + '\\n'))\n",
    "        returns[\"model_summary\"] = ''.join(s)\n",
    "        \n",
    "        # Informaci√≥n de la arquitectura\n",
    "        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy a tipos nativos de Python para JSON serialization\n",
    "        total_params = model.count_params()\n",
    "        trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "        \n",
    "        # Convertir a tipos nativos de Python\n",
    "        if hasattr(total_params, 'item'):\n",
    "            total_params = int(total_params.item())\n",
    "        else:\n",
    "            total_params = int(total_params)\n",
    "        \n",
    "        if hasattr(trainable_params, 'item'):\n",
    "            trainable_params = int(trainable_params.item())\n",
    "        else:\n",
    "            trainable_params = int(trainable_params)\n",
    "        \n",
    "        returns[\"model_architecture\"] = {\n",
    "            \"input_shape\": str(model.input_shape) if hasattr(model, 'input_shape') else \"N/A\",\n",
    "            \"output_shape\": str(model.output_shape) if hasattr(model, 'output_shape') else \"N/A\",\n",
    "            \"total_params\": total_params,  # Ya convertido a int nativo\n",
    "            \"trainable_params\": trainable_params  # Ya convertido a int nativo\n",
    "        }\n",
    "        \n",
    "        # Informaci√≥n de capas\n",
    "        returns[\"layers\"] = []\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            # Obtener output_shape de manera segura\n",
    "            output_shape = \"N/A\"\n",
    "            try:\n",
    "                if hasattr(layer, 'output') and layer.output is not None:\n",
    "                    try:\n",
    "                        output_shape = str(layer.output.shape)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if output_shape == \"N/A\":\n",
    "                    if hasattr(layer, 'get_config'):\n",
    "                        config = layer.get_config()\n",
    "                        if 'output_shape' in config:\n",
    "                            output_shape = str(config['output_shape'])\n",
    "                \n",
    "                if output_shape == \"N/A\":\n",
    "                    if callable(getattr(layer, 'compute_output_shape', None)):\n",
    "                        if i == 0 and hasattr(model, 'input_shape') and model.input_shape:\n",
    "                            computed = layer.compute_output_shape(model.input_shape)\n",
    "                            output_shape = str(computed)\n",
    "                        elif hasattr(layer, 'input_shape') and layer.input_shape:\n",
    "                            computed = layer.compute_output_shape(layer.input_shape)\n",
    "                            output_shape = str(computed)\n",
    "            except Exception:\n",
    "                output_shape = \"N/A\"\n",
    "            \n",
    "            # Obtener par√°metros de manera segura\n",
    "            params = 0\n",
    "            try:\n",
    "                params_raw = layer.count_params()\n",
    "                # ‚ö†Ô∏è CR√çTICO: Convertir a tipo nativo de Python para JSON serialization\n",
    "                if hasattr(params_raw, 'item'):\n",
    "                    params = int(params_raw.item())\n",
    "                else:\n",
    "                    params = int(params_raw)\n",
    "            except Exception:\n",
    "                params = 0\n",
    "            \n",
    "            returns[\"layers\"].append({\n",
    "                \"index\": i,  # Ya es int nativo\n",
    "                \"name\": layer.name,\n",
    "                \"type\": type(layer).__name__,\n",
    "                \"output_shape\": output_shape,\n",
    "                \"params\": params  # Ya convertido a int nativo\n",
    "            })\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test summary\n",
    "# ‚ö†Ô∏è NOTA: Solo funciona si tienes modelo disponible\n",
    "\n",
    "try:\n",
    "    if 'model' not in globals():\n",
    "        print(\"‚ö†Ô∏è  No hay modelo disponible. Ejecuta primero init() y fit()\")\n",
    "    else:\n",
    "        model_summary = summary(model)\n",
    "        print(\"üìä Resumen del modelo:\")\n",
    "        print(json.dumps(model_summary, indent=2, default=str))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Checklist Antes de Guardar\n",
    "\n",
    "Antes de guardar el notebook para publicaci√≥n, verifica:\n",
    "\n",
    "- [ ] **Configuraci√≥n del modelo**: Actualiza `APP_NAME`, `MODEL_TYPE`, `USE_CASE`, `VERSION` en la celda de imports\n",
    "- [ ] **Metadata de celdas**: Todas las funciones requeridas tienen el metadata correcto:\n",
    "  - [ ] `init()` ‚Üí metadata `\"name\": \"mltkc_init\"`\n",
    "  - [ ] `fit()` ‚Üí metadata `\"name\": \"mltkc_stage_create_model_fit\"`\n",
    "  - [ ] `apply()` ‚Üí metadata `\"name\": \"mltkc_stage_create_model_apply\"`\n",
    "  - [ ] `summary()` ‚Üí metadata `\"name\": \"mltkc_summary\"`\n",
    "  - [ ] `save()` ‚Üí metadata `\"name\": \"mltkc_save\"`\n",
    "  - [ ] `load()` ‚Üí metadata `\"name\": \"mltkc_load\"` (opcional)\n",
    "- [ ] **Funciones requeridas**: Todas las funciones tienen las firmas correctas\n",
    "- [ ] **Sin errores**: Ejecuta \"Cell ‚Üí Run All\" para verificar que no hay errores\n",
    "- [ ] **Nombre del notebook**: Sigue la convenci√≥n `{app_name}_{model_type}_{use_case}_{version}.ipynb`\n",
    "\n",
    "**Para m√°s informaci√≥n**: Consulta la **Gu√≠a Completa Data Scientist E2E** para detalles sobre metadata y exportaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Recursos Adicionales\n",
    "\n",
    "- **Gu√≠a Completa Data Scientist E2E**: `GUIA_COMPLETA_DATA_SCIENTIST_E2E.md`\n",
    "- **Troubleshooting**: `TROUBLESHOOTING.md` - Soluci√≥n de problemas comunes\n",
    "- **Diagn√≥stico de Telemetr√≠a**: `DIAGNOSTICO_TELEMETRIA.md`\n",
    "- **Documentaci√≥n DSDL**: https://docs.splunk.com/Documentation/DSDL\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Pr√≥ximos Pasos\n",
    "\n",
    "Una vez que el modelo est√° publicado:\n",
    "\n",
    "1. **Monitorear m√©tricas**: Revisar `index=ml_metrics` regularmente\n",
    "2. **Ajustar thresholds**: Modificar percentil de anomal√≠as seg√∫n necesidad\n",
    "3. **Crear dashboards**: Visualizar anomal√≠as en tiempo real\n",
    "4. **Configurar alertas**: Alertar cuando anomal√≠as superen umbral\n",
    "5. **Refinar modelo**: Iterar con m√°s datos o arquitecturas diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Celdas adicionales para desarrollo\n",
    "# Puedes agregar aqu√≠ celdas adicionales para pruebas, visualizaciones, etc.\n",
    "# Estas celdas NO se exportan al archivo .py\n",
    "\n",
    "print(\"üí° Usa esta √°rea para c√≥digo de desarrollo que no debe exportarse\")\n",
    "print(\"   Ejemplo: an√°lisis exploratorio, visualizaciones, pruebas adicionales\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
