{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Splunk App for Data Science and Deep Learning - Notebook for Autoencoder with TensorFlow and Keras (version 2.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Example\n",
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk App for Data Science and Deep Learning (DSDL). As an example we use a custom autoencoder built on keras and tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# mltkc_import\n",
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importar helpers empresariales\n",
    "import sys\n",
    "sys.path.append('/dltk/notebooks_custom/helpers')\n",
    "\n",
    "from telemetry_helper import log_metrics, log_training_step, log_error\n",
    "from metrics_calculator import calculate_all_metrics\n",
    "from preprocessor import standard_preprocessing, apply_preprocessing\n",
    "\n",
    "# Global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "\n",
    "# Configuraci√≥n del modelo (usando naming est√°ndar)\n",
    "APP_NAME = \"app1\"\n",
    "MODEL_TYPE = \"autoencoder\"\n",
    "USE_CASE = \"demo_anomalias\"\n",
    "VERSION = \"v1\"\n",
    "MODEL_NAME = f\"{APP_NAME}_{MODEL_TYPE}_{USE_CASE}_{VERSION}\"\n",
    "\n",
    "print(f\"üì¶ Modelo configurado: {MODEL_NAME}\")\n",
    "print(f\"‚úÖ Helpers empresariales importados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"Keras version: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUERY DATA FROM SPLUNK\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Exploraci√≥n de datos\n",
    "from dsdlsupport import SplunkSearch\n",
    "\n",
    "# Obtener muestra de datos para exploraci√≥n\n",
    "print(\"üîç Obteniendo muestra de datos de Splunk...\")\n",
    "search = SplunkSearch.SplunkSearch(\n",
    "    search='index=demo_anomalias_data | head 1000 | table feature_*'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA SPLUNK TO DATA FRAME\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Informaci√≥n b√°sica\n",
    "df_eda = search.as_df()\n",
    "print(f\"‚úÖ Datos obtenidos: {df_eda.shape[0]} filas, {df_eda.shape[1]} columnas\")\n",
    "df_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 Informaci√≥n B√°sica del Dataset\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Informaci√≥n b√°sica\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMACI√ìN B√ÅSICA DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Dimensiones: {df_eda.shape}\")\n",
    "print(f\"\\nüìã Columnas: {list(df_eda.columns)}\")\n",
    "print(f\"\\nüìà Tipos de datos:\\n{df_eda.dtypes}\")\n",
    "print(f\"\\nüìâ Informaci√≥n completa:\")\n",
    "df_eda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 Estad√≠sticas Descriptivas\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Estad√≠sticas descriptivas\n",
    "print(\"=\" * 60)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 60)\n",
    "print(df_eda.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.4 Detecci√≥n de Valores Faltantes\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Valores faltantes\n",
    "print(\"=\" * 60)\n",
    "print(\"VALORES FALTANTES\")\n",
    "print(\"=\" * 60)\n",
    "missing = df_eda.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"‚ö†Ô∏è Se encontraron valores faltantes:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores faltantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.5 Visualizaciones B√°sicas\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('An√°lisis Exploratorio de Datos - demo_anomalias_data', fontsize=16)\n",
    "\n",
    "# Histogramas de las primeras 4 features\n",
    "for i, col in enumerate(df_eda.columns[:4]):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    df_eda[col].hist(bins=50, ax=ax, alpha=0.7)\n",
    "    ax.set_title(f'Distribuci√≥n de {col}')\n",
    "    ax.set_xlabel('Valor')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.6 Matriz de Correlaci√≥n\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Correlaciones\n",
    "import numpy as np\n",
    "\n",
    "# Calcular matriz de correlaci√≥n\n",
    "corr_matrix = df_eda.corr()\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci√≥n - Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.7 Conclusiones del EDA\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - EDA: Conclusiones\n",
    "print(\"=\" * 60)\n",
    "print(\"CONCLUSIONES DEL EDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Obtener solo columnas num√©ricas para an√°lisis\n",
    "numeric_cols = df_eda.select_dtypes(include=[np.number]).columns\n",
    "df_numeric = df_eda[numeric_cols]\n",
    "\n",
    "print(\"‚úÖ Dimensiones del dataset:\", df_eda.shape)\n",
    "print(\"   - Filas (muestras):\", df_eda.shape[0])\n",
    "print(\"   - Columnas (features):\", df_eda.shape[1])\n",
    "print(\"   - Features num√©ricas:\", len(df_numeric.columns))\n",
    "\n",
    "print(\"\\n‚úÖ Valores faltantes:\", df_eda.isnull().sum().sum())\n",
    "if df_eda.isnull().sum().sum() > 0:\n",
    "    print(\"   ‚ö†Ô∏è  Hay valores faltantes que necesitamos manejar\")\n",
    "\n",
    "# Rango de valores (solo para columnas num√©ricas)\n",
    "if len(df_numeric.columns) > 0:\n",
    "    min_val = df_numeric.min().min()\n",
    "    max_val = df_numeric.max().max()\n",
    "    print(f\"\\n‚úÖ Rango de valores (num√©ricos):\")\n",
    "    print(f\"   - M√≠nimo: {min_val:.2f}\")\n",
    "    print(f\"   - M√°ximo: {max_val:.2f}\")\n",
    "    print(f\"   - Rango total: {max_val - min_val:.2f}\")\n",
    "    \n",
    "    # Verificar si necesitamos normalizaci√≥n\n",
    "    std_values = df_numeric.std()\n",
    "    if std_values.max() / std_values.min() > 10:\n",
    "        print(\"   ‚ö†Ô∏è  Hay features con escalas muy diferentes ‚Üí Normalizaci√≥n REQUERIDA\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Escalas similares ‚Üí Normalizaci√≥n recomendada\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No se encontraron columnas num√©ricas\")\n",
    "\n",
    "print(\"\\nüìù Decisiones para el modelo basadas en EDA:\")\n",
    "print(\"   - Features a usar: Todas las num√©ricas disponibles\")\n",
    "print(\"   - Preprocesamiento: Normalizaci√≥n (StandardScaler)\")\n",
    "print(\"   - Arquitectura: Autoencoder simple (input ‚Üí encoding ‚Üí output)\")\n",
    "print(\"   - Encoding dimension: ~10% del input dimension (ajustable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Verificar helpers\n",
    "print(\"üîç Verificando helpers empresariales...\")\n",
    "\n",
    "try:\n",
    "    from telemetry_helper import log_metrics\n",
    "    print(\"‚úÖ telemetry_helper importado\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando telemetry_helper: {e}\")\n",
    "\n",
    "try:\n",
    "    from metrics_calculator import calculate_all_metrics\n",
    "    print(\"‚úÖ metrics_calculator importado\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando metrics_calculator: {e}\")\n",
    "\n",
    "try:\n",
    "    from preprocessor import standard_preprocessing\n",
    "    print(\"‚úÖ preprocessor importado\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando preprocessor: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Todos los helpers est√°n disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_init",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_init\n",
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "\n",
    "def init(df, param):\n",
    "    \"\"\"\n",
    "    Inicializar autoencoder para detecci√≥n de anomal√≠as.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos de Splunk\n",
    "        param: Diccionario con par√°metros del modelo\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo Keras compilado\n",
    "    \"\"\"\n",
    "    print(f\"üîß Inicializando modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    # Obtener features del DataFrame\n",
    "    if 'feature_variables' in param:\n",
    "        feature_cols = param['feature_variables']\n",
    "    else:\n",
    "        # Si no hay feature_variables definidas, usar todas las num√©ricas\n",
    "        feature_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "        if not feature_cols:\n",
    "            # Fallback: buscar columnas que empiecen con 'feature_'\n",
    "            feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "    \n",
    "    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"üìä Shape de los datos: {X.shape}\")\n",
    "    print(f\"üìã Features seleccionadas: {len(X.columns)}\")\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # Par√°metros del modelo (con valores por defecto)\n",
    "    encoding_dim = 10  # Dimensi√≥n de la capa oculta (bottleneck)\n",
    "    if 'options' in param and 'params' in param['options']:\n",
    "        if 'encoding_dim' in param['options']['params']:\n",
    "            encoding_dim = int(param['options']['params']['encoding_dim'])\n",
    "        if 'components' in param['options']['params']:\n",
    "            encoding_dim = int(param['options']['params']['components'])\n",
    "    \n",
    "    activation = 'relu'\n",
    "    if 'options' in param and 'params' in param['options']:\n",
    "        if 'activation' in param['options']['params']:\n",
    "            activation = param['options']['params']['activation']\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Par√°metros del modelo:\")\n",
    "    print(f\"   - Input dimension: {input_dim}\")\n",
    "    print(f\"   - Encoding dimension: {encoding_dim}\")\n",
    "    print(f\"   - Activation: {activation}\")\n",
    "    \n",
    "    # Construir autoencoder\n",
    "    # Encoder\n",
    "    encoder = keras.layers.Dense(\n",
    "        encoding_dim, \n",
    "        activation=activation,\n",
    "        input_shape=(input_dim,),\n",
    "        name='encoder'\n",
    "    )\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = keras.layers.Dense(\n",
    "        input_dim,\n",
    "        activation=activation,\n",
    "        name='decoder'\n",
    "    )\n",
    "    \n",
    "    # Modelo completo\n",
    "    model = keras.Sequential([\n",
    "        encoder,\n",
    "        decoder\n",
    "    ], name='Autoencoder')\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',  # Mean Squared Error para autoencoder\n",
    "        metrics=['mae']  # Mean Absolute Error como m√©trica adicional\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Modelo compilado exitosamente\")\n",
    "    print(f\"üìê Arquitectura: {input_dim} ‚Üí {encoding_dim} ‚Üí {input_dim}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test init localmente\n",
    "# Crear datos dummy para probar\n",
    "test_df = pd.DataFrame({\n",
    "    'feature_0': np.random.randn(100),\n",
    "    'feature_1': np.random.randn(100),\n",
    "    'feature_2': np.random.randn(100),\n",
    "    'feature_3': np.random.randn(100),\n",
    "    'feature_4': np.random.randn(100)\n",
    "})\n",
    "\n",
    "test_param = {\n",
    "    'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "    'options': {\n",
    "        'params': {\n",
    "            'encoding_dim': 10\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "test_model = init(test_df, test_param)\n",
    "print(\"\\nüìä Resumen del modelo:\")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_fit",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_fit\n",
    "# returns a fit info json object\n",
    "\n",
    "def fit(model, df, param):\n",
    "    \"\"\"\n",
    "    Entrenar autoencoder con telemetr√≠a autom√°tica.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras inicializado\n",
    "        df: DataFrame con datos de entrenamiento\n",
    "        param: Diccionario con par√°metros de entrenamiento\n",
    "    \n",
    "    Returns:\n",
    "        dict: Informaci√≥n del entrenamiento (historial, m√©tricas, etc.)\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Iniciando entrenamiento del modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    returns = {}\n",
    "    \n",
    "    # Obtener features\n",
    "    if 'feature_variables' in param:\n",
    "        feature_cols = param['feature_variables']\n",
    "    else:\n",
    "        feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        if not feature_cols:\n",
    "            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"üìä Datos de entrenamiento: {X.shape[0]} muestras, {X.shape[1]} features\")\n",
    "    \n",
    "    # Preprocesamiento: Normalizaci√≥n\n",
    "    print(\"üîß Aplicando preprocesamiento (normalizaci√≥n)...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    # Guardar scaler en returns para uso posterior\n",
    "    returns['scaler'] = scaler\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    validation_split = 0.2\n",
    "    \n",
    "    if 'options' in param and 'params' in param['options']:\n",
    "        if 'epochs' in param['options']['params']:\n",
    "            epochs = int(param['options']['params']['epochs'])\n",
    "        if 'batch_size' in param['options']['params']:\n",
    "            batch_size = int(param['options']['params']['batch_size'])\n",
    "        if 'validation_split' in param['options']['params']:\n",
    "            validation_split = float(param['options']['params']['validation_split'])\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Par√°metros de entrenamiento:\")\n",
    "    print(f\"   - Epochs: {epochs}\")\n",
    "    print(f\"   - Batch size: {batch_size}\")\n",
    "    print(f\"   - Validation split: {validation_split}\")\n",
    "    \n",
    "    # Callback para TensorBoard (opcional)\n",
    "    log_dir = f\"/srv/notebooks/logs/fit/{MODEL_NAME}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "    \n",
    "    # Callback personalizado para logging de telemetr√≠a\n",
    "    class TelemetryCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            \"\"\"Enviar m√©tricas de cada √©poca a Splunk\"\"\"\n",
    "            logs = logs or {}\n",
    "            try:\n",
    "                # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy/Pandas a tipos nativos de Python para JSON serialization\n",
    "                # Los valores int64/float64 de NumPy no son serializables a JSON directamente\n",
    "                epoch_value = int(epoch + 1)  # Convertir a int nativo\n",
    "                loss_value = float(logs.get('loss', 0)) if logs.get('loss') is not None else 0.0\n",
    "                val_loss_value = float(logs.get('val_loss', 0)) if logs.get('val_loss') is not None else 0.0\n",
    "                mae_value = float(logs.get('mae', 0)) if logs.get('mae') is not None else 0.0\n",
    "                val_mae_value = float(logs.get('val_mae', 0)) if logs.get('val_mae') is not None else 0.0\n",
    "                \n",
    "                log_training_step(\n",
    "                    model_name=MODEL_NAME,\n",
    "                    epoch=epoch_value,\n",
    "                    loss=loss_value,\n",
    "                    val_loss=val_loss_value,\n",
    "                    mae=mae_value,\n",
    "                    val_mae=val_mae_value\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error enviando telemetr√≠a en √©poca {epoch + 1}: {e}\")\n",
    "                import traceback\n",
    "                print(f\"   Traceback completo: {traceback.format_exc()}\")\n",
    "    \n",
    "    telemetry_callback = TelemetryCallback()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nüèãÔ∏è  Iniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        x=X_scaled_df,\n",
    "        y=X_scaled_df,  # Autoencoder: input = output\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_callback, telemetry_callback]\n",
    "    )\n",
    "    \n",
    "    returns['fit_history'] = history\n",
    "    returns['model_epochs'] = epochs\n",
    "    returns['model_batch_size'] = batch_size\n",
    "    returns['scaler'] = scaler  # Guardar scaler para uso en apply\n",
    "    \n",
    "    # Evaluar modelo en datos completos\n",
    "    print(\"\\nüìä Evaluando modelo en datos completos...\")\n",
    "    test_results = model.evaluate(X_scaled_df, X_scaled_df, verbose=0)\n",
    "    returns['model_loss'] = test_results[0]\n",
    "    returns['model_mae'] = test_results[1] if len(test_results) > 1 else None\n",
    "    \n",
    "    print(f\"‚úÖ Entrenamiento completado\")\n",
    "    print(f\"   - Loss final: {test_results[0]:.6f}\")\n",
    "    if len(test_results) > 1:\n",
    "        print(f\"   - MAE final: {test_results[1]:.6f}\")\n",
    "    \n",
    "    # Calcular m√©tricas de reconstrucci√≥n\n",
    "    print(\"\\nüìà Calculando m√©tricas de reconstrucci√≥n...\")\n",
    "    X_pred = model.predict(X_scaled_df, verbose=0)\n",
    "    \n",
    "    # Calcular MSE y RMSE\n",
    "    mse = mean_squared_error(X_scaled_df.values, X_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    returns['mse'] = float(mse)\n",
    "    returns['rmse'] = float(rmse)\n",
    "    \n",
    "    print(f\"   - MSE: {mse:.6f}\")\n",
    "    print(f\"   - RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    # Enviar m√©tricas finales a Splunk (telemetr√≠a)\n",
    "    try:\n",
    "        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy/Pandas a tipos nativos de Python para JSON serialization\n",
    "        # Los valores int64/float64 de NumPy no son serializables a JSON directamente\n",
    "        mae_value = float(returns['model_mae']) if returns['model_mae'] is not None else None\n",
    "        rmse_value = float(rmse) if rmse is not None else None\n",
    "        mse_value = float(mse) if mse is not None else None\n",
    "        loss_value = float(test_results[0]) if test_results[0] is not None else None\n",
    "        \n",
    "        log_metrics(\n",
    "            model_name=MODEL_NAME,\n",
    "            r2_score=None,  # Autoencoder no tiene R¬≤ tradicional\n",
    "            mae=mae_value,\n",
    "            rmse=rmse_value,\n",
    "            mse=mse_value,\n",
    "            loss=loss_value,\n",
    "            app_name=APP_NAME,\n",
    "            model_version=VERSION,\n",
    "            project=USE_CASE\n",
    "        )\n",
    "        print(\"‚úÖ M√©tricas enviadas a Splunk\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error enviando m√©tricas a Splunk: {e}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback completo: {traceback.format_exc()}\")\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test fit localmente\n",
    "# Usar datos dummy m√°s grandes\n",
    "test_df_fit = pd.DataFrame({\n",
    "    'feature_0': np.random.randn(500),\n",
    "    'feature_1': np.random.randn(500),\n",
    "    'feature_2': np.random.randn(500),\n",
    "    'feature_3': np.random.randn(500),\n",
    "    'feature_4': np.random.randn(500)\n",
    "})\n",
    "\n",
    "test_param_fit = {\n",
    "    'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "    'options': {\n",
    "        'params': {\n",
    "            'epochs': '10',  # Pocas √©pocas para prueba r√°pida\n",
    "            'batch_size': '32',\n",
    "            'validation_split': '0.2'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Crear modelo de prueba\n",
    "test_model_fit = init(test_df_fit, test_param_fit)\n",
    "\n",
    "# Entrenar (esto puede tomar unos minutos)\n",
    "print(\"‚è≥ Entrenando modelo de prueba (esto tomar√° unos minutos)...\")\n",
    "fit_results = fit(test_model_fit, test_df_fit, test_param_fit)\n",
    "\n",
    "print(\"\\n‚úÖ Test de fit completado exitosamente\")\n",
    "print(f\"   - Loss: {fit_results.get('model_loss', 'N/A')}\")\n",
    "print(f\"   - MSE: {fit_results.get('mse', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_apply",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_apply\n",
    "\n",
    "def apply(model, df, param):\n",
    "    \"\"\"\n",
    "    Aplicar autoencoder para detecci√≥n de anomal√≠as.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras entrenado\n",
    "        df: DataFrame con datos nuevos para inferencia\n",
    "        param: Diccionario con par√°metros (debe contener scaler de fit)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con reconstrucciones y scores de anomal√≠a\n",
    "    \"\"\"\n",
    "    print(f\"üîÆ Aplicando modelo: {MODEL_NAME}\")\n",
    "    \n",
    "    # Obtener features (debe coincidir con las usadas en fit)\n",
    "    if 'feature_variables' in param:\n",
    "        feature_cols = param['feature_variables']\n",
    "    else:\n",
    "        feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        if not feature_cols:\n",
    "            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"üìä Datos de inferencia: {X.shape[0]} muestras, {X.shape[1]} features\")\n",
    "    \n",
    "    # Obtener scaler del entrenamiento (desde param o fit_results)\n",
    "    scaler = None\n",
    "    if 'scaler' in param:\n",
    "        scaler = param['scaler']\n",
    "    elif hasattr(model, 'scaler'):\n",
    "        scaler = model.scaler\n",
    "    \n",
    "    # Aplicar normalizaci√≥n\n",
    "    if scaler is not None:\n",
    "        # Usar scaler del entrenamiento\n",
    "        X_scaled = scaler.transform(X)\n",
    "        print(\"‚úÖ Usando scaler del entrenamiento\")\n",
    "    else:\n",
    "        # Crear nuevo scaler si no est√° disponible (fallback)\n",
    "        print(\"‚ö†Ô∏è  Scaler no encontrado en param. Aplicando normalizaci√≥n nueva...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "    # Predecir reconstrucciones\n",
    "    print(\"üîÑ Calculando reconstrucciones...\")\n",
    "    X_reconstructed = model.predict(X_scaled_df, verbose=0)\n",
    "    X_reconstructed_df = pd.DataFrame(X_reconstructed, columns=X.columns, index=X.index)\n",
    "    \n",
    "    # Calcular error de reconstrucci√≥n (MSE por muestra)\n",
    "    reconstruction_error = np.mean((X_scaled_df.values - X_reconstructed_df.values) ** 2, axis=1)\n",
    "    \n",
    "    # Calcular threshold para anomal√≠as (percentil 95)\n",
    "    # En producci√≥n, este threshold deber√≠a venir del conjunto de entrenamiento\n",
    "    anomaly_threshold = np.percentile(reconstruction_error, 95)\n",
    "    \n",
    "    # Detectar anomal√≠as\n",
    "    is_anomaly = reconstruction_error > anomaly_threshold\n",
    "    anomaly_score = reconstruction_error / (anomaly_threshold + 1e-10)  # Normalizar score\n",
    "    \n",
    "    print(f\"üìä Estad√≠sticas de reconstrucci√≥n:\")\n",
    "    print(f\"   - Error medio: {np.mean(reconstruction_error):.6f}\")\n",
    "    print(f\"   - Error mediano: {np.median(reconstruction_error):.6f}\")\n",
    "    print(f\"   - Threshold (percentil 95): {anomaly_threshold:.6f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {np.sum(is_anomaly)} / {len(is_anomaly)} ({100*np.mean(is_anomaly):.2f}%)\")\n",
    "    \n",
    "    # Construir DataFrame de resultados\n",
    "    results = pd.DataFrame({\n",
    "        'reconstruction_error': reconstruction_error,\n",
    "        'anomaly_score': anomaly_score,\n",
    "        'is_anomaly': is_anomaly.astype(int)\n",
    "    }, index=X.index)\n",
    "    \n",
    "    # Agregar reconstrucciones como columnas\n",
    "    for i, col in enumerate(X.columns):\n",
    "        results[f'reconstruction_{col}'] = X_reconstructed_df[col].values\n",
    "        results[f'original_{col}'] = X[col].values\n",
    "    \n",
    "    print(f\"‚úÖ Inferencia completada\")\n",
    "    print(f\"   - Shape de resultados: {results.shape}\")\n",
    "    \n",
    "    # Enviar telemetr√≠a de inferencia a Splunk\n",
    "    try:\n",
    "        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy/Pandas a tipos nativos de Python para JSON serialization\n",
    "        # Los valores int64/float64 de NumPy no son serializables a JSON directamente\n",
    "        \n",
    "        # IMPORTANTE: Usar .item() para convertir scalars NumPy a tipos nativos de Python\n",
    "        # Esto es m√°s robusto que int() o float() porque maneja todos los tipos NumPy\n",
    "        num_predictions = int(len(df))  # len() ya retorna int nativo\n",
    "        \n",
    "        # Para valores NumPy, usar .item() si est√° disponible, sino usar int()/float()\n",
    "        if hasattr(is_anomaly.sum(), 'item'):\n",
    "            num_anomalies = int(is_anomaly.sum().item())\n",
    "        else:\n",
    "            num_anomalies = int(is_anomaly.sum())\n",
    "        \n",
    "        if hasattr(reconstruction_error.mean(), 'item'):\n",
    "            avg_reconstruction_error = float(reconstruction_error.mean().item())\n",
    "        else:\n",
    "            avg_reconstruction_error = float(reconstruction_error.mean())\n",
    "        \n",
    "        if hasattr(anomaly_threshold, 'item'):\n",
    "            anomaly_threshold_native = float(anomaly_threshold.item())\n",
    "        else:\n",
    "            anomaly_threshold_native = float(anomaly_threshold)\n",
    "        \n",
    "        # ‚ö†Ô∏è DIAGN√ìSTICO: Verificar que todos los valores son serializables a JSON\n",
    "        # Esto ayuda a identificar problemas antes de pasarlos al helper\n",
    "        # En apply(), ANTES de llamar a log_metrics/log_prediction, agregar:\n",
    "        import json\n",
    "        \n",
    "        # Preparar todos los valores convertidos\n",
    "        telemetry_data = {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"num_predictions\": int(len(df)),\n",
    "            \"num_anomalies\": int(is_anomaly.sum()),\n",
    "            \"avg_reconstruction_error\": float(reconstruction_error.mean()),\n",
    "            \"anomaly_threshold\": float(anomaly_threshold),\n",
    "            \"app_name\": APP_NAME,\n",
    "            \"model_version\": VERSION,\n",
    "            \"project\": USE_CASE\n",
    "        }\n",
    "        \n",
    "        # Eliminar valores None\n",
    "        telemetry_data = {k: v for k, v in telemetry_data.items() if v is not None}\n",
    "        \n",
    "        # INTENTAR serializar a JSON para verificar que todos los valores son serializables\n",
    "        try:\n",
    "            json.dumps(telemetry_data)\n",
    "            print(\"‚úÖ Todos los valores son serializables a JSON\")\n",
    "        except TypeError as e:\n",
    "            print(f\"‚ùå ERROR DE SERIALIZACI√ìN: {e}\")\n",
    "            print(f\"   Valores problem√°ticos:\")\n",
    "            for k, v in telemetry_data.items():\n",
    "                try:\n",
    "                    json.dumps({k: v})\n",
    "                except TypeError:\n",
    "                    print(f\"      - {k}: {type(v)} = {v}\")\n",
    "                    # Convertir cualquier valor NumPy restante\n",
    "                    if hasattr(v, 'item'):  # Es un scalar NumPy\n",
    "                        telemetry_data[k] = v.item()\n",
    "                    elif isinstance(v, (np.integer, np.floating)):\n",
    "                        telemetry_data[k] = float(v) if isinstance(v, np.floating) else int(v)\n",
    "            \n",
    "            # Intentar de nuevo\n",
    "            try:\n",
    "                json.dumps(telemetry_data)\n",
    "                print(\"‚úÖ Valores corregidos, ahora son serializables\")\n",
    "            except TypeError as e2:\n",
    "                print(f\"‚ùå ERROR PERSISTENTE: {e2}\")\n",
    "                raise  # Re-lanzar el error para que se capture en el except externo\n",
    "        \n",
    "        # ‚ö†Ô∏è CR√çTICO: Usar log_metrics directamente (ya est√° importado al inicio del notebook)\n",
    "        # NO re-importar log_metrics aqu√≠ porque causa UnboundLocalError\n",
    "        # Intentar usar log_prediction si est√° disponible, sino usar log_metrics directamente\n",
    "        try:\n",
    "            # Intentar importar log_prediction si est√° disponible\n",
    "            try:\n",
    "                from telemetry_helper import log_prediction\n",
    "                # Si log_prediction existe, usarlo\n",
    "                log_prediction(\n",
    "                    model_name=telemetry_data[\"model_name\"],\n",
    "                    num_predictions=telemetry_data[\"num_predictions\"],\n",
    "                    num_anomalies=telemetry_data[\"num_anomalies\"],\n",
    "                    avg_reconstruction_error=telemetry_data[\"avg_reconstruction_error\"],\n",
    "                    anomaly_threshold=telemetry_data[\"anomaly_threshold\"],\n",
    "                    app_name=telemetry_data[\"app_name\"],\n",
    "                    model_version=telemetry_data[\"model_version\"],\n",
    "                    owner=OWNER if 'OWNER' in globals() else None,\n",
    "                    project=telemetry_data[\"project\"]\n",
    "                )\n",
    "                print(\"‚úÖ Telemetr√≠a de inferencia enviada a Splunk (usando log_prediction)\")\n",
    "            except ImportError:\n",
    "                # Si log_prediction no existe, usar log_metrics directamente (ya est√° importado)\n",
    "                # NO re-importar log_metrics aqu√≠ porque ya est√° importado al inicio del notebook\n",
    "                log_metrics(\n",
    "                    model_name=telemetry_data[\"model_name\"],\n",
    "                    num_predictions=telemetry_data[\"num_predictions\"],\n",
    "                    num_anomalies=telemetry_data[\"num_anomalies\"],\n",
    "                    avg_reconstruction_error=telemetry_data[\"avg_reconstruction_error\"],\n",
    "                    anomaly_threshold=telemetry_data[\"anomaly_threshold\"],\n",
    "                    app_name=telemetry_data[\"app_name\"],\n",
    "                    model_version=telemetry_data[\"model_version\"],\n",
    "                    project=telemetry_data[\"project\"]\n",
    "                )\n",
    "                print(\"‚úÖ Telemetr√≠a de inferencia enviada a Splunk (usando log_metrics)\")\n",
    "        except Exception as telemetry_error:\n",
    "            # Capturar cualquier otro error de telemetr√≠a (no solo ImportError)\n",
    "            print(f\"‚ö†Ô∏è  Error en telemetr√≠a (despu√©s de verificaci√≥n JSON): {telemetry_error}\")\n",
    "            import traceback\n",
    "            print(f\"   Traceback: {traceback.format_exc()}\")\n",
    "            # No re-lanzar el error para que apply() pueda continuar\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error enviando telemetr√≠a de inferencia a Splunk: {e}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback completo: {traceback.format_exc()}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test apply localmente\n",
    "# Crear datos nuevos para inferencia\n",
    "test_df_apply = pd.DataFrame({\n",
    "    'feature_0': np.random.randn(100),\n",
    "    'feature_1': np.random.randn(100),\n",
    "    'feature_2': np.random.randn(100),\n",
    "    'feature_3': np.random.randn(100),\n",
    "    'feature_4': np.random.randn(100)\n",
    "})\n",
    "\n",
    "# Agregar scaler al param (simulando que viene de fit)\n",
    "test_param_apply = {\n",
    "    'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "    'scaler': fit_results.get('scaler')  # Usar scaler del fit anterior\n",
    "}\n",
    "\n",
    "# Aplicar modelo\n",
    "results = apply(test_model_fit, test_df_apply, test_param_apply)\n",
    "\n",
    "print(\"\\nüìä Primeras 10 filas de resultados:\")\n",
    "print(results.head(10))\n",
    "\n",
    "print(\"\\nüìà Estad√≠sticas de anomal√≠as:\")\n",
    "print(f\"   - Total muestras: {len(results)}\")\n",
    "print(f\"   - Anomal√≠as detectadas: {results['is_anomaly'].sum()}\")\n",
    "print(f\"   - Porcentaje: {100 * results['is_anomaly'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_summary",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "\n",
    "def summary(model=None):\n",
    "    \"\"\"\n",
    "    Proporcionar metadatos y resumen del modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras (opcional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadatos del modelo\n",
    "    \"\"\"\n",
    "    returns = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"app_name\": APP_NAME,\n",
    "        \"model_type\": MODEL_TYPE,\n",
    "        \"use_case\": USE_CASE,\n",
    "        \"version\": VERSION,\n",
    "        \"version_info\": {\n",
    "            \"tensorflow\": tf.__version__,\n",
    "            \"keras\": keras.__version__,\n",
    "            \"numpy\": np.__version__,\n",
    "            \"pandas\": pd.__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if model is not None:\n",
    "        # Guardar resumen del modelo como string\n",
    "        s = []\n",
    "        model.summary(print_fn=lambda x: s.append(x + '\\n'))\n",
    "        returns[\"model_summary\"] = ''.join(s)\n",
    "        \n",
    "        # Informaci√≥n de la arquitectura\n",
    "        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy a tipos nativos de Python para JSON serialization\n",
    "        # DSDL serializa el resultado de summary() a JSON, y valores NumPy causan errores\n",
    "        total_params = model.count_params()\n",
    "        trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "        \n",
    "        # Convertir a tipos nativos de Python\n",
    "        if hasattr(total_params, 'item'):\n",
    "            total_params = int(total_params.item())\n",
    "        else:\n",
    "            total_params = int(total_params)\n",
    "        \n",
    "        if hasattr(trainable_params, 'item'):\n",
    "            trainable_params = int(trainable_params.item())\n",
    "        else:\n",
    "            trainable_params = int(trainable_params)\n",
    "        \n",
    "        returns[\"model_architecture\"] = {\n",
    "            \"input_shape\": str(model.input_shape) if hasattr(model, 'input_shape') else \"N/A\",\n",
    "            \"output_shape\": str(model.output_shape) if hasattr(model, 'output_shape') else \"N/A\",\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params\n",
    "        }\n",
    "        \n",
    "        # Informaci√≥n de capas\n",
    "        returns[\"layers\"] = []\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            # Obtener output_shape de manera segura\n",
    "            output_shape = \"N/A\"\n",
    "            try:\n",
    "                # En Keras 2.x/TensorFlow 2.x, intentar m√∫ltiples m√©todos\n",
    "                if hasattr(layer, 'output') and layer.output is not None:\n",
    "                    # M√©todo 1: Desde el tensor output (disponible despu√©s de build)\n",
    "                    try:\n",
    "                        output_shape = str(layer.output.shape)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if output_shape == \"N/A\":\n",
    "                    # M√©todo 2: Intentar obtener desde config\n",
    "                    if hasattr(layer, 'get_config'):\n",
    "                        config = layer.get_config()\n",
    "                        if 'output_shape' in config:\n",
    "                            output_shape = str(config['output_shape'])\n",
    "                \n",
    "                if output_shape == \"N/A\":\n",
    "                    # M√©todo 3: Calcular si es posible\n",
    "                    if callable(getattr(layer, 'compute_output_shape', None)):\n",
    "                        # Necesitamos input_shape, intentar obtenerlo\n",
    "                        if i == 0 and hasattr(model, 'input_shape') and model.input_shape:\n",
    "                            # Primera capa: usar input_shape del modelo\n",
    "                            computed = layer.compute_output_shape(model.input_shape)\n",
    "                            output_shape = str(computed)\n",
    "                        elif hasattr(layer, 'input_shape') and layer.input_shape:\n",
    "                            # Capas intermedias: usar input_shape de la capa\n",
    "                            computed = layer.compute_output_shape(layer.input_shape)\n",
    "                            output_shape = str(computed)\n",
    "            except Exception:\n",
    "                # Si todo falla, usar \"N/A\"\n",
    "                output_shape = \"N/A\"\n",
    "            \n",
    "            # Obtener par√°metros de manera segura\n",
    "            params = 0\n",
    "            try:\n",
    "                params_raw = layer.count_params()\n",
    "                # ‚ö†Ô∏è CR√çTICO: Convertir a tipo nativo de Python para JSON serialization\n",
    "                if hasattr(params_raw, 'item'):\n",
    "                    params = int(params_raw.item())\n",
    "                else:\n",
    "                    params = int(params_raw)\n",
    "            except Exception:\n",
    "                params = 0\n",
    "            \n",
    "            returns[\"layers\"].append({\n",
    "                \"index\": i,\n",
    "                \"name\": layer.name,\n",
    "                \"type\": type(layer).__name__,\n",
    "                \"output_shape\": output_shape,\n",
    "                \"params\": params\n",
    "            })\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - Test summary\n",
    "model_summary = summary(test_model_fit)\n",
    "print(\"üìä Resumen del modelo:\")\n",
    "print(json.dumps(model_summary, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 8 - save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_save",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_save\n",
    "# Funci√≥n REQUERIDA: DSDL llama a save(model, name) despu√©s de fit()\n",
    "\n",
    "def save(model, name):\n",
    "    \"\"\"\n",
    "    Guardar modelo Keras en disco.\n",
    "    \n",
    "    IMPORTANTE: Esta funci√≥n es llamada autom√°ticamente por DSDL despu√©s de fit().\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo Keras entrenado (retornado por fit())\n",
    "        name: Nombre del modelo (pasado por DSDL desde \"into app:model_name\")\n",
    "    \n",
    "    Returns:\n",
    "        model: Retorna el modelo (requerido por DSDL)\n",
    "    \"\"\"\n",
    "    # Importar os si no est√° disponible (para cuando DSDL exporta el m√≥dulo)\n",
    "    import os\n",
    "    \n",
    "    # Asegurar que el directorio existe\n",
    "    os.makedirs(MODEL_DIRECTORY, exist_ok=True)\n",
    "    \n",
    "    # Guardar modelo Keras\n",
    "    filepath = MODEL_DIRECTORY + name + \".keras\"\n",
    "    model.save(filepath)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo guardado en: {filepath}\")\n",
    "    print(f\"üìä Tama√±o del archivo: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # NOTA: Si tienes un scaler u otros objetos, gu√°rdalos tambi√©n\n",
    "    # Ejemplo: si el scaler est√° en el modelo o en globals\n",
    "    # from sklearn.externals import joblib  # o import joblib\n",
    "    # if hasattr(model, 'scaler'):\n",
    "    #     joblib.dump(model.scaler, MODEL_DIRECTORY + name + \"_scaler.pkl\")\n",
    "    \n",
    "    # DSDL espera que retornes el modelo\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.2 Probar Funci√≥n `save()` Localmente\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - Test save localmente\n",
    "print(\"üíæ Probando funci√≥n save()...\")\n",
    "\n",
    "# Verificar que las variables necesarias existen\n",
    "if 'test_model_fit' not in globals():\n",
    "    print(\"‚ö†Ô∏è  test_model_fit no est√° definido.\")\n",
    "    print(\"   Necesitas ejecutar primero el test de fit() (Paso 6.2)\")\n",
    "    print(\"   Para crear un modelo de prueba r√°pido, ejecuta:\")\n",
    "    print(\"\"\"\n",
    "    # Crear datos dummy\n",
    "    test_df_fit = pd.DataFrame({\n",
    "        'feature_0': np.random.randn(100),\n",
    "        'feature_1': np.random.randn(100),\n",
    "        'feature_2': np.random.randn(100),\n",
    "        'feature_3': np.random.randn(100),\n",
    "        'feature_4': np.random.randn(100)\n",
    "    })\n",
    "    test_param_fit = {\n",
    "        'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],\n",
    "        'options': {'params': {'epochs': '5', 'batch_size': '32'}}\n",
    "    }\n",
    "    test_model_fit = init(test_df_fit, test_param_fit)\n",
    "    fit_results = fit(test_model_fit, test_df_fit, test_param_fit)\n",
    "    \"\"\")\n",
    "else:\n",
    "    try:\n",
    "        # Asegurar que MODEL_DIRECTORY est√° definido\n",
    "        try:\n",
    "            model_dir = MODEL_DIRECTORY\n",
    "        except NameError:\n",
    "            model_dir = \"/srv/app/model/data/\"\n",
    "        \n",
    "        # Guardar modelo de prueba usando la firma correcta\n",
    "        saved_model = save(test_model_fit, name=\"test_autoencoder\")\n",
    "        print(f\"‚úÖ Modelo guardado exitosamente\")\n",
    "        \n",
    "        # Verificar que el archivo existe\n",
    "        filepath = model_dir + \"test_autoencoder.keras\"\n",
    "        if os.path.exists(filepath):\n",
    "            file_size = os.path.getsize(filepath) / (1024 * 1024)\n",
    "            print(f\"üìä Tama√±o del archivo: {file_size:.2f} MB\")\n",
    "            print(f\"‚úÖ Archivo creado correctamente: {filepath}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Archivo no encontrado: {filepath}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 9 - load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "mltkc_load",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mltkc_load\n",
    "# Funci√≥n opcional para cargar modelo guardado durante desarrollo\n",
    "# DSDL NO llama a esta funci√≥n autom√°ticamente\n",
    "\n",
    "def load(name):\n",
    "    \"\"\"\n",
    "    Cargar modelo Keras desde disco.\n",
    "    \n",
    "    √ötil para desarrollo local o pruebas.\n",
    "    DSDL NO usa esta funci√≥n autom√°ticamente.\n",
    "    \n",
    "    Args:\n",
    "        name: Nombre del archivo (sin extensi√≥n)\n",
    "    \n",
    "    Returns:\n",
    "        Model: Modelo Keras cargado\n",
    "    \"\"\"\n",
    "    # Importar os si no est√° disponible\n",
    "    import os\n",
    "    \n",
    "    # Asegurar que MODEL_DIRECTORY est√° definido (usar variable global o local)\n",
    "    try:\n",
    "        # Intentar usar MODEL_DIRECTORY global\n",
    "        model_dir = MODEL_DIRECTORY\n",
    "    except NameError:\n",
    "        # Si no existe, usar valor por defecto\n",
    "        model_dir = \"/srv/app/model/data/\"\n",
    "    \n",
    "    filepath = model_dir + name + \".keras\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"‚ùå Archivo no encontrado: {filepath}\")\n",
    "    \n",
    "    print(f\"üì• Cargando modelo desde: {filepath}\")\n",
    "    model = keras.models.load_model(filepath)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo cargado exitosamente\")\n",
    "    print(f\"üìä Arquitectura: {model.input_shape} ‚Üí {model.output_shape}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 9.3 Probar Funci√≥n `load()` Localmente (Opcional)\n",
    "\n",
    "# THIS CELL IS NOT EXPORTED - Test load localmente (opcional)\n",
    "print(\"üì• Probando funci√≥n load()...\")\n",
    "\n",
    "# Verificar que el archivo existe antes de intentar cargarlo\n",
    "import os\n",
    "\n",
    "# Asegurar que MODEL_DIRECTORY est√° definido\n",
    "try:\n",
    "    model_dir = MODEL_DIRECTORY\n",
    "except NameError:\n",
    "    model_dir = \"/srv/app/model/data/\"\n",
    "\n",
    "test_filepath = model_dir + \"test_autoencoder.keras\"\n",
    "if not os.path.exists(test_filepath):\n",
    "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {test_filepath}\")\n",
    "    print(\"   Necesitas ejecutar primero el test de save() (Paso 8.2)\")\n",
    "    print(\"   O aseg√∫rate de que test_model_fit existe y ejecuta:\")\n",
    "    print(\"   saved_model = save(test_model_fit, name='test_autoencoder')\")\n",
    "else:\n",
    "    try:\n",
    "        loaded_model = load(\"test_autoencoder\")\n",
    "        print(\"‚úÖ Modelo cargado exitosamente\")\n",
    "        \n",
    "        # Verificar que son equivalentes (solo si test_model_fit existe)\n",
    "        if 'test_model_fit' in globals():\n",
    "            print(\"\\nüîç Verificando que el modelo cargado funciona...\")\n",
    "            test_input = np.random.randn(1, 5)  # 5 features\n",
    "            output_original = test_model_fit.predict(test_input, verbose=0)\n",
    "            output_loaded = loaded_model.predict(test_input, verbose=0)\n",
    "            \n",
    "            if np.allclose(output_original, output_loaded):\n",
    "                print(\"‚úÖ Los modelos producen resultados id√©nticos\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Los modelos producen resultados diferentes\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  test_model_fit no est√° definido, no se puede verificar equivalencia\")\n",
    "            print(\"   Pero el modelo se carg√≥ correctamente ‚úÖ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
