{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Modelo Dummy Completo - Demo DSDL\n",
        "\n",
        "**Objetivo**: Demo completo del flujo Dev → Prod con modelo funcional\n",
        "\n",
        "**Modelo**: Autoencoder para detección de anomalías\n",
        "\n",
        "**Nota**: Este es un modelo de DEMO para validar el flujo completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necesarios\n",
        "import sys\n",
        "sys.path.append(\"/srv/notebooks_custom/helpers\")\n",
        "\n",
        "# Helpers custom\n",
        "from telemetry_helper import log_metrics, log_training_step, log_error, log_prediction\n",
        "from metrics_calculator import calculate_all_metrics\n",
        "\n",
        "# Librerías estándar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"✅ Todos los imports exitosos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 1: INIT - Inicialización del modelo\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def init(param):\n",
        "    \"\"\"\n",
        "    Inicializar modelo y parámetros globales\n",
        "    \n",
        "    Args:\n",
        "        param: Diccionario con parámetros del modelo\n",
        "    \n",
        "    Returns:\n",
        "        model: Modelo inicializado\n",
        "    \"\"\"\n",
        "    global model, scaler, n_features\n",
        "    \n",
        "    try:\n",
        "        print(f\"🔧 Inicializando modelo con parámetros: {param}\")\n",
        "        n_features = None\n",
        "        model = None\n",
        "        scaler = None\n",
        "        print(\"✅ Modelo inicializado (autoencoder para detección de anomalías)\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='init')\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 2: FIT - Entrenamiento del modelo\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def fit(df, param):\n",
        "    \"\"\"\n",
        "    Entrenar modelo con datos\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame con datos de entrenamiento\n",
        "        param: Diccionario con parámetros de entrenamiento\n",
        "    \n",
        "    Returns:\n",
        "        model: Modelo entrenado\n",
        "    \"\"\"\n",
        "    global model, scaler, n_features\n",
        "    \n",
        "    try:\n",
        "        print(f\"📊 Datos recibidos: {df.shape}\")\n",
        "        \n",
        "        # Detectar features\n",
        "        feature_cols = df.columns.tolist()\n",
        "        n_features = len(feature_cols)\n",
        "        X = df[feature_cols].values\n",
        "        \n",
        "        # Preprocesamiento\n",
        "        print(\"🔧 Preprocesando datos...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        \n",
        "        # Split train/val\n",
        "        X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # Construir modelo (autoencoder)\n",
        "        print(f\"🔧 Construyendo autoencoder con {n_features} features...\")\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(32, activation='relu', input_shape=(n_features,)),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(8, activation='relu'),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(n_features, activation='sigmoid')\n",
        "        ])\n",
        "        \n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=param.get('learning_rate', 0.001)),\n",
        "            loss='mse',\n",
        "            metrics=['mae', 'mse']\n",
        "        )\n",
        "        \n",
        "        # Entrenar\n",
        "        epochs = param.get('epochs', 50)\n",
        "        batch_size = param.get('batch_size', 32)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train, X_train,\n",
        "            validation_data=(X_val, X_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        "        )\n",
        "        \n",
        "        # Calcular métricas\n",
        "        val_pred = model.predict(X_val, verbose=0)\n",
        "        mse = np.mean((X_val - val_pred) ** 2)\n",
        "        mae = np.mean(np.abs(X_val - val_pred))\n",
        "        \n",
        "        print(f\"📊 MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
        "        \n",
        "        # Telemetría\n",
        "        log_metrics(model_name='demo_modelo_completo', mae=mae, mse=mse)\n",
        "        log_training_step(model_name='demo_modelo_completo', epoch=epochs, loss=mse)\n",
        "        \n",
        "        print(\"✅ Modelo entrenado exitosamente\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='fit')\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 3: APPLY - Aplicar modelo a nuevos datos\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def apply(df):\n",
        "    \"\"\"\n",
        "    Aplicar modelo entrenado para predicción\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame con datos para predecir\n",
        "    \n",
        "    Returns:\n",
        "        errors: Array con errores de reconstrucción\n",
        "    \"\"\"\n",
        "    global model, scaler\n",
        "    \n",
        "    try:\n",
        "        if model is None or scaler is None:\n",
        "            raise ValueError(\"Modelo no entrenado. Ejecuta fit() primero\")\n",
        "        \n",
        "        feature_cols = df.columns.tolist()\n",
        "        X = df[feature_cols].values\n",
        "        X_scaled = scaler.transform(X)\n",
        "        \n",
        "        reconstructed = model.predict(X_scaled, verbose=0)\n",
        "        errors = np.mean((X_scaled - reconstructed) ** 2, axis=1)\n",
        "        \n",
        "        log_prediction(model_name='demo_modelo_completo', num_predictions=len(errors))\n",
        "        \n",
        "        return errors\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='apply')\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 4: SUMMARY - Resumen del modelo\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def summary(df):\n",
        "    \"\"\"\n",
        "    Generar resumen de métricas del modelo\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame (opcional)\n",
        "    \n",
        "    Returns:\n",
        "        summary_dict: Diccionario con resumen del modelo\n",
        "    \"\"\"\n",
        "    global model, scaler\n",
        "    \n",
        "    try:\n",
        "        if model is None:\n",
        "            return {\"status\": \"Model not initialized\"}\n",
        "        \n",
        "        return {\n",
        "            \"model_type\": \"Autoencoder\",\n",
        "            \"use_case\": \"Anomaly Detection\",\n",
        "            \"trainable_parameters\": model.count_params(),\n",
        "            \"layers\": len(model.layers),\n",
        "            \"features\": n_features if 'n_features' in globals() else None\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='summary')\n",
        "        return {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Flujo Completo: Splunk → Modelo → Producción\n",
        "\n",
        "**Este notebook tiene 2 modos de uso:**\n",
        "\n",
        "1. **Modo Testing Local** (abajo): Con datos sintéticos\n",
        "2. **Modo Producción con Splunk**: Las funciones `fit()` y `apply()` reciben datos automáticamente de Splunk\n",
        "\n",
        "### ⚠️ IMPORTANTE: Las funciones `init`, `fit`, `apply`, `summary` NO consultan Splunk manualmente\n",
        "### DSDL automáticamente les pasa los datos cuando ejecutas:\n",
        "### `| fit MLTKContainer algo=demo_modelo_completo ... into app:modelo_v1`\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Opcional: Consultar Splunk Interactivamente (Para Exploración)\n",
        "\n",
        "Si quieres consultar datos de Splunk desde el notebook (modo interactivo de exploración):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPCIONAL: Consultar Splunk interactivamente (solo para exploración/debug)\n",
        "# ⚠️ Esto NO es necesario para producción, DSDL ya lo hace automáticamente\n",
        "\n",
        "try:\n",
        "    from dsdlsupport import SplunkSearch\n",
        "    \n",
        "    # Consultar datos de Splunk\n",
        "    search = SplunkSearch.SplunkSearch(\n",
        "        search='index=demo_anomalias_data | head 100 | table feature_*'\n",
        "    )\n",
        "    \n",
        "    # Obtener como DataFrame\n",
        "    df_sample = search.as_df()\n",
        "    \n",
        "    print(f\"✅ Muestra de Splunk: {df_sample.shape}\")\n",
        "    print(f\"   Columnas: {df_sample.columns.tolist()}\")\n",
        "    print(\"\\nPrimeras 5 filas:\")\n",
        "    print(df_sample.head())\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️  SplunkSearch no disponible o no configurado: {e}\")\n",
        "    print(\"   (Esto es normal si no tienes Splunk Access configurado)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar datos dummy para testing\n",
        "np.random.seed(42)\n",
        "\n",
        "# Datos normales\n",
        "normal_data = np.random.normal(0, 1, (1000, 5))\n",
        "df_normal = pd.DataFrame(normal_data, columns=[f'feature_{i}' for i in range(5)])\n",
        "\n",
        "# Anomalías\n",
        "anomaly_data = np.random.normal(5, 2, (100, 5))\n",
        "df_anomaly = pd.DataFrame(anomaly_data, columns=[f'feature_{i}' for i in range(5)])\n",
        "\n",
        "df_test = pd.concat([df_normal, df_anomaly], ignore_index=True)\n",
        "\n",
        "print(f\"✅ Datos: {df_test.shape} (1000 normales, 100 anómalos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test completo: INIT → FIT → APPLY → SUMMARY\n",
        "print(\"=\" * 60)\n",
        "print(\"🧪 TEST COMPLETO DEL MODELO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "param = {'epochs': 20, 'batch_size': 32, 'learning_rate': 0.001}\n",
        "\n",
        "# 1. INIT\n",
        "print(\"\\n1️⃣  INIT\")\n",
        "model = init(param)\n",
        "\n",
        "# 2. FIT\n",
        "print(\"\\n2️⃣  FIT\")\n",
        "model = fit(df_normal, param)\n",
        "\n",
        "# 3. APPLY\n",
        "print(\"\\n3️⃣  APPLY\")\n",
        "predictions = apply(df_test)\n",
        "print(f\"✅ Predicciones: {len(predictions)} valores\")\n",
        "\n",
        "# 4. SUMMARY\n",
        "print(\"\\n4️⃣  SUMMARY\")\n",
        "summary_result = summary(df_test)\n",
        "print(f\"✅ Summary: {summary_result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ TEST COMPLETO EXITOSO\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
