{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Modelo Demo - Flujo Completo Cristian-Style\n",
        "\n",
        "**Objetivo**: Replicar el flujo real de trabajo de Cristian (DS)\n",
        "\n",
        "**Modelo**: Autoencoder para detección de anomalías\n",
        "\n",
        "**Flujo**: Splunk → EDA → Preprocesamiento → Modelo → Entrenar → Inferir → Telemetría\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# IMPORTS - Librerías necesarias\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/srv/notebooks_custom/helpers\")\n",
        "\n",
        "# Helpers custom empresariales\n",
        "from telemetry_helper import log_metrics, log_training_step, log_error, log_prediction\n",
        "from metrics_calculator import calculate_all_metrics\n",
        "from preprocessor import standard_preprocessing\n",
        "\n",
        "# Librerías estándar ML/DL\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"✅ Todos los imports exitosos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 1: INIT - Inicialización del modelo\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def init(param):\n",
        "    \"\"\"\n",
        "    Inicializar modelo y parámetros globales\n",
        "    \n",
        "    Args:\n",
        "        param: Diccionario con parámetros del modelo\n",
        "    \n",
        "    Returns:\n",
        "        model: Modelo inicializado\n",
        "    \"\"\"\n",
        "    global model, scaler, n_features\n",
        "    \n",
        "    try:\n",
        "        print(f\"🔧 Inicializando modelo con parámetros: {param}\")\n",
        "        n_features = None\n",
        "        model = None\n",
        "        scaler = None\n",
        "        print(\"✅ Modelo inicializado (autoencoder para detección de anomalías)\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='init')\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 2: FIT - Entrenamiento del modelo\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def fit(df, param):\n",
        "    \"\"\"\n",
        "    Entrenar modelo con datos\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame con datos de entrenamiento\n",
        "        param: Diccionario con parámetros de entrenamiento\n",
        "    \n",
        "    Returns:\n",
        "        model: Modelo entrenado\n",
        "    \"\"\"\n",
        "    global model, scaler, n_features\n",
        "    \n",
        "    try:\n",
        "        print(f\"📊 Datos recibidos: {df.shape}\")\n",
        "        \n",
        "        # Detectar features\n",
        "        feature_cols = df.columns.tolist()\n",
        "        n_features = len(feature_cols)\n",
        "        X = df[feature_cols].values\n",
        "        \n",
        "        # Preprocesamiento\n",
        "        print(\"🔧 Preprocesando datos...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        \n",
        "        # Split train/val\n",
        "        X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # Construir modelo (autoencoder)\n",
        "        print(f\"🔧 Construyendo autoencoder con {n_features} features...\")\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(32, activation='relu', input_shape=(n_features,)),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(8, activation='relu'),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(n_features, activation='sigmoid')\n",
        "        ])\n",
        "        \n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=param.get('learning_rate', 0.001)),\n",
        "            loss='mse',\n",
        "            metrics=['mae', 'mse']\n",
        "        )\n",
        "        \n",
        "        # Entrenar\n",
        "        epochs = param.get('epochs', 50)\n",
        "        batch_size = param.get('batch_size', 32)\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train, X_train,\n",
        "            validation_data=(X_val, X_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        "        )\n",
        "        \n",
        "        # Calcular métricas\n",
        "        val_pred = model.predict(X_val, verbose=0)\n",
        "        mse = np.mean((X_val - val_pred) ** 2)\n",
        "        mae = np.mean(np.abs(X_val - val_pred))\n",
        "        \n",
        "        print(f\"📊 MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
        "        \n",
        "        # Telemetría\n",
        "        log_metrics(model_name='demo_modelo_completo', mae=mae, mse=mse)\n",
        "        log_training_step(model_name='demo_modelo_completo', epoch=epochs, loss=mse)\n",
        "        \n",
        "        print(\"✅ Modelo entrenado exitosamente\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='fit')\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 3: APPLY - Aplicar modelo a nuevos datos\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def apply(df):\n",
        "    \"\"\"\n",
        "    Aplicar modelo entrenado para predicción\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame con datos para predecir\n",
        "    \n",
        "    Returns:\n",
        "        errors: Array con errores de reconstrucción\n",
        "    \"\"\"\n",
        "    global model, scaler\n",
        "    \n",
        "    try:\n",
        "        if model is None or scaler is None:\n",
        "            raise ValueError(\"Modelo no entrenado. Ejecuta fit() primero\")\n",
        "        \n",
        "        feature_cols = df.columns.tolist()\n",
        "        X = df[feature_cols].values\n",
        "        X_scaled = scaler.transform(X)\n",
        "        \n",
        "        reconstructed = model.predict(X_scaled, verbose=0)\n",
        "        errors = np.mean((X_scaled - reconstructed) ** 2, axis=1)\n",
        "        \n",
        "        log_prediction(model_name='demo_modelo_completo', num_predictions=len(errors))\n",
        "        \n",
        "        return errors\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='apply')\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FUNCIÓN 4: SUMMARY - Resumen del modelo\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "def summary(df):\n",
        "    \"\"\"\n",
        "    Generar resumen de métricas del modelo\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame (opcional)\n",
        "    \n",
        "    Returns:\n",
        "        summary_dict: Diccionario con resumen del modelo\n",
        "    \"\"\"\n",
        "    global model, scaler\n",
        "    \n",
        "    try:\n",
        "        if model is None:\n",
        "            return {\"status\": \"Model not initialized\"}\n",
        "        \n",
        "        return {\n",
        "            \"model_type\": \"Autoencoder\",\n",
        "            \"use_case\": \"Anomaly Detection\",\n",
        "            \"trainable_parameters\": model.count_params(),\n",
        "            \"layers\": len(model.layers),\n",
        "            \"features\": n_features if 'n_features' in globals() else None\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_error(model_name='demo_modelo_completo', error_message=str(e), error_type='summary')\n",
        "        return {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# FASE 1: EXPLORACIÓN DE DATOS DESDE SPLUNK\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "## Cómo Cristian trabajaría:\n",
        "1. Consultar datos de Splunk (modo exploratorio)\n",
        "2. Exploración rápida de los datos (EDA)\n",
        "3. Seleccionar features relevantes\n",
        "4. Definir preprocesamiento\n",
        "5. Crear y entrenar modelo\n",
        "6. Telemetría automática\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consultar datos de Splunk (modo exploratorio/interactivo)\n",
        "try:\n",
        "    from dsdlsupport import SplunkSearch\n",
        "    \n",
        "    print(\"🔍 Consultando datos de Splunk...\")\n",
        "    # Cristian consultaría sus datos reales\n",
        "    search = SplunkSearch.SplunkSearch(\n",
        "        search='index=demo_anomalias_data | head 500 | table feature_*'\n",
        "    )\n",
        "    \n",
        "    # Obtener como DataFrame\n",
        "    df = search.as_df()\n",
        "    \n",
        "    print(f\"✅ Datos cargados: {df.shape}\")\n",
        "    print(f\"   Columnas: {df.columns.tolist()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️  SplunkSearch no disponible: {e}\")\n",
        "    print(\"   Generando datos dummy para continuar...\")\n",
        "    \n",
        "    # Fallback: datos dummy (como backup)\n",
        "    np.random.seed(42)\n",
        "    normal_data = np.random.normal(0, 1, (500, 5))\n",
        "    anomaly_data = np.random.normal(5, 2, (50, 5))\n",
        "    df = pd.DataFrame(\n",
        "        np.vstack([normal_data, anomaly_data]), \n",
        "        columns=[f'feature_{i}' for i in range(5)]\n",
        "    )\n",
        "    print(f\"✅ Datos dummy generados: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploración rápida de los datos\n",
        "print(\"📊 Información del dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n📈 Estadísticas descriptivas:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n🔍 Primeras filas:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización rápida (EDA como Cristian)\n",
        "plt.figure(figsize=(12, 8))\n",
        "df.plot(subplots=True, figsize=(10, 6*len(df.columns)), sharex=True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Exploración completada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# NOTA IMPORTANTE\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "**Las funciones `init`, `fit`, `apply`, `summary` están arriba**\n",
        "\n",
        "**Cuando Cristian ejecuta:**\n",
        "- `| fit MLTKContainer algo=demo_modelo_completo epochs=20 from feature_* into app:demo_v1`\n",
        "- DSDL automáticamente llama a `fit(df, param)` con los datos de Splunk\n",
        "  \n",
        "**NO necesitas consultar Splunk manualmente en fit/apply**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
