# Gu√≠a Completa Data Scientist: Workflow End-to-End DSDL

**Objetivo**: Gu√≠a paso a paso para que un Data Scientist cree, desarrolle y publique un modelo de autoencoder para detecci√≥n de anomal√≠as usando Splunk DSDL.

**Duraci√≥n estimada**: 2-3 horas  
**Nivel**: Intermedio  
**Requisitos**: Acceso a JupyterLab, datos indexados en Splunk (`demo_anomalias_data`)

---

## üéØ Contexto del Proyecto

### Objetivo del Modelo
Crear un **autoencoder** para detectar anomal√≠as en datos industriales indexados en Splunk.

### Datos
- **Index**: `demo_anomalias_data`
- **Campos**: `feature_*` (feature_0, feature_1, feature_2, etc.)
- **Formato**: Series temporales de sensores industriales

### Naming Est√°ndar
Seguiremos la convenci√≥n: `{app_name}_{model_type}_{use_case}_{version}`

**Ejemplo**: `app1_autoencoder_demo_anomalias_v1`

### Qu√© Obtendremos al Final
Al guardar el notebook en JupyterLab, DSDL **autom√°ticamente**:
1. Exporta las funciones `init`, `fit`, `apply`, `summary` a un m√≥dulo Python
2. Publica el modelo en `/srv/app/model/`
3. Hace disponible el modelo para usar desde SPL con `| fit` y `| apply`

---

## üìã Checklist Pre-Workflow

Antes de comenzar, verifica que tienes:

- [ ] Acceso a JupyterLab (desde DSDL ‚Üí Containers ‚Üí Open JupyterLab)
- [ ] Contenedor DEV iniciado y corriendo
- [ ] Datos indexados en `demo_anomalias_data`
- [ ] Permisos para usar los helpers empresariales
- [ ] Template empresarial base disponible

### ‚ö†Ô∏è Nota Importante: C√≥mo Copiar C√≥digo

**Cuando copies c√≥digo de esta gu√≠a a JupyterLab**:
- ‚úÖ Copia SOLO el c√≥digo dentro de los bloques ```python
- ‚ùå NO copies los triple backticks ``` ni el ``` al final
- ‚úÖ Pega directamente en una nueva c√©lula de c√≥digo
- ‚úÖ Ejecuta la c√©lula con Shift+Enter

**Ejemplo correcto**:
```python
# Esto es lo que debes copiar
print("Hola mundo")
```

**Ejemplo incorrecto** (NO copies esto):
```
```python
print("Hola mundo")
```
```

---

## üìù Paso 1: Preparar el Notebook Base

### 1.1 Abrir JupyterLab

1. En Splunk Web, navega a: **DSDL ‚Üí Containers**
2. Si no hay contenedor DEV activo, haz clic en **"Start Development Container"**
3. Espera a que el contenedor inicie (1-2 minutos)
4. Haz clic en **"Open JupyterLab"**
5. Se abre JupyterLab en una nueva pesta√±a del navegador

### 1.2 Cargar Template Empresarial

1. En JupyterLab, navega a: `/dltk/notebooks_custom/`
2. Abre el archivo: `template_empresa_base.ipynb`
3. Haz clic en: **File ‚Üí Save As...**
4. Guarda como: `app1_autoencoder_demo_anomalias_v1.ipynb`
5. **IMPORTANTE**: Este nombre sigue la convenci√≥n est√°ndar

**Ubicaci√≥n final**: `/dltk/notebooks_custom/app1_autoencoder_demo_anomalias_v1.ipynb`

### 1.3 Verificar Estructura del Template

El template deber√≠a tener estas secciones base:
- C√©lulas de importaci√≥n
- Funci√≥n `init(df, param)`
- Funci√≥n `fit(model, df, param)`
- Funci√≥n `apply(model, df, param)`
- Funci√≥n `summary(model)`

**Si no tiene estas funciones, las agregaremos en los siguientes pasos.**

---

## üîç Paso 2: Exploraci√≥n de Datos (EDA)

### 2.1 Obtener Muestra de Datos de Splunk

En una nueva c√©lula al inicio del notebook (despu√©s de las importaciones), agrega:

```python
# THIS CELL IS NOT EXPORTED - EDA: Exploraci√≥n de datos
from dsdlsupport import SplunkSearch

# Obtener muestra de datos para exploraci√≥n
print("üîç Obteniendo muestra de datos de Splunk...")
search = SplunkSearch.SplunkSearch(
    search='index=demo_anomalias_data | head 1000 | table feature_*'
)

df_eda = search.as_df()
print(f"‚úÖ Datos obtenidos: {df_eda.shape[0]} filas, {df_eda.shape[1]} columnas")
df_eda.head()
```

**Ejecuta esta c√©lula** y verifica que obtienes datos.

### 2.2 Informaci√≥n B√°sica del Dataset

```python
# THIS CELL IS NOT EXPORTED - EDA: Informaci√≥n b√°sica
print("=" * 60)
print("INFORMACI√ìN B√ÅSICA DEL DATASET")
print("=" * 60)
print(f"\nüìä Dimensiones: {df_eda.shape}")
print(f"\nüìã Columnas: {list(df_eda.columns)}")
print(f"\nüìà Tipos de datos:\n{df_eda.dtypes}")
print(f"\nüìâ Informaci√≥n completa:")
df_eda.info()
```

### 2.3 Estad√≠sticas Descriptivas

```python
# THIS CELL IS NOT EXPORTED - EDA: Estad√≠sticas descriptivas
print("=" * 60)
print("ESTAD√çSTICAS DESCRIPTIVAS")
print("=" * 60)
print(df_eda.describe())
```

### 2.4 Detecci√≥n de Valores Faltantes

```python
# THIS CELL IS NOT EXPORTED - EDA: Valores faltantes
print("=" * 60)
print("VALORES FALTANTES")
print("=" * 60)
missing = df_eda.isnull().sum()
if missing.sum() > 0:
    print("‚ö†Ô∏è Se encontraron valores faltantes:")
    print(missing[missing > 0])
else:
    print("‚úÖ No hay valores faltantes")
```

### 2.5 Visualizaciones B√°sicas

```python
# THIS CELL IS NOT EXPORTED - EDA: Visualizaciones
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar estilo
plt.style.use('seaborn-v0_8-darkgrid')
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('An√°lisis Exploratorio de Datos - demo_anomalias_data', fontsize=16)

# Histogramas de las primeras 4 features
for i, col in enumerate(df_eda.columns[:4]):
    ax = axes[i // 2, i % 2]
    df_eda[col].hist(bins=50, ax=ax, alpha=0.7)
    ax.set_title(f'Distribuci√≥n de {col}')
    ax.set_xlabel('Valor')
    ax.set_ylabel('Frecuencia')

plt.tight_layout()
plt.show()
```

### 2.6 Matriz de Correlaci√≥n

```python
# THIS CELL IS NOT EXPORTED - EDA: Correlaciones
import numpy as np

# Calcular matriz de correlaci√≥n
corr_matrix = df_eda.corr()

# Visualizar
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Matriz de Correlaci√≥n - Features')
plt.tight_layout()
plt.show()
```

### 2.7 Conclusiones del EDA

**Objetivo de esta secci√≥n**: Resumir los hallazgos clave del an√°lisis exploratorio para tomar decisiones informadas sobre el modelo.

**Conclusiones que queremos obtener**:

1. **Dimensiones del dataset**: ¬øCu√°ntas muestras y features tenemos?
   - Define si necesitamos muestreo o si tenemos suficientes datos
   - Determina la arquitectura del modelo (input dimension)

2. **Tipos de datos**: ¬øQu√© tipos de datos tenemos?
   - Determina qu√© features usar (solo num√©ricas para autoencoder)
   - Identifica si hay columnas categ√≥ricas que necesiten encoding

3. **Valores faltantes**: ¬øHay datos faltantes?
   - Determina estrategia de preprocesamiento (imputaci√≥n, eliminaci√≥n)
   - Afecta la calidad del modelo

4. **Rangos de valores**: ¬øQu√© rangos tienen nuestras features?
   - Determina si necesitamos normalizaci√≥n (valores en escalas muy diferentes)
   - Autoencoders funcionan mejor con datos normalizados

5. **Distribuciones**: ¬øC√≥mo est√°n distribuidas las features?
   - Identifica outliers (afectan el entrenamiento del autoencoder)
   - Determina si necesitamos transformaciones (log, etc.)

6. **Correlaciones**: ¬øHay features altamente correlacionadas?
   - Puede indicar redundancia (puede simplificar el modelo)
   - Afecta la interpretaci√≥n del modelo

**C√≥digo para conclusiones**:

```python
# THIS CELL IS NOT EXPORTED - EDA: Conclusiones
print("=" * 60)
print("CONCLUSIONES DEL EDA")
print("=" * 60)

# Obtener solo columnas num√©ricas para an√°lisis
numeric_cols = df_eda.select_dtypes(include=[np.number]).columns
df_numeric = df_eda[numeric_cols]

print("‚úÖ Dimensiones del dataset:", df_eda.shape)
print("   - Filas (muestras):", df_eda.shape[0])
print("   - Columnas (features):", df_eda.shape[1])
print("   - Features num√©ricas:", len(df_numeric.columns))

print("\n‚úÖ Valores faltantes:", df_eda.isnull().sum().sum())
if df_eda.isnull().sum().sum() > 0:
    print("   ‚ö†Ô∏è  Hay valores faltantes que necesitamos manejar")

# Rango de valores (solo para columnas num√©ricas)
if len(df_numeric.columns) > 0:
    min_val = df_numeric.min().min()
    max_val = df_numeric.max().max()
    print(f"\n‚úÖ Rango de valores (num√©ricos):")
    print(f"   - M√≠nimo: {min_val:.2f}")
    print(f"   - M√°ximo: {max_val:.2f}")
    print(f"   - Rango total: {max_val - min_val:.2f}")
    
    # Verificar si necesitamos normalizaci√≥n
    std_values = df_numeric.std()
    if std_values.max() / std_values.min() > 10:
        print("   ‚ö†Ô∏è  Hay features con escalas muy diferentes ‚Üí Normalizaci√≥n REQUERIDA")
    else:
        print("   ‚úÖ Escalas similares ‚Üí Normalizaci√≥n recomendada")
else:
    print("\n‚ö†Ô∏è  No se encontraron columnas num√©ricas")

print("\nüìù Decisiones para el modelo basadas en EDA:")
print("   - Features a usar: Todas las num√©ricas disponibles")
print("   - Preprocesamiento: Normalizaci√≥n (StandardScaler)")
print("   - Arquitectura: Autoencoder simple (input ‚Üí encoding ‚Üí output)")
print("   - Encoding dimension: ~10% del input dimension (ajustable)")
```

**‚úÖ Validaci√≥n**: Aseg√∫rate de haber ejecutado todas las c√©lulas EDA y haber entendido tus datos antes de continuar.

---

## üèóÔ∏è Paso 3: Configurar Imports y Helpers Empresariales

### 3.1 Actualizar C√©lula de Imports

En la primera c√©lula marcada con `# mltkc_import`, reemplaza o agrega:

```python
# mltkc_import
# this definition exposes all python module imports that should be available in all subsequent commands

import json
import os
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
import keras
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# Importar helpers empresariales
import sys
sys.path.append('/dltk/notebooks_custom/helpers')

from telemetry_helper import log_metrics, log_training_step, log_error
from metrics_calculator import calculate_all_metrics
from preprocessor import standard_preprocessing, apply_preprocessing

# Global constants
MODEL_DIRECTORY = "/srv/app/model/data/"

# Configuraci√≥n del modelo (usando naming est√°ndar)
APP_NAME = "app1"
MODEL_TYPE = "autoencoder"
USE_CASE = "demo_anomalias"
VERSION = "v1"
MODEL_NAME = f"{APP_NAME}_{MODEL_TYPE}_{USE_CASE}_{VERSION}"

print(f"üì¶ Modelo configurado: {MODEL_NAME}")
print(f"‚úÖ Helpers empresariales importados correctamente")
```

**Ejecuta esta c√©lula** y verifica que no hay errores de importaci√≥n.

### 3.2 Verificar Helpers Disponibles

```python
# THIS CELL IS NOT EXPORTED - Verificar helpers
print("üîç Verificando helpers empresariales...")

try:
    from telemetry_helper import log_metrics
    print("‚úÖ telemetry_helper importado")
except ImportError as e:
    print(f"‚ùå Error importando telemetry_helper: {e}")

try:
    from metrics_calculator import calculate_all_metrics
    print("‚úÖ metrics_calculator importado")
except ImportError as e:
    print(f"‚ùå Error importando metrics_calculator: {e}")

try:
    from preprocessor import standard_preprocessing
    print("‚úÖ preprocessor importado")
except ImportError as e:
    print(f"‚ùå Error importando preprocessor: {e}")

print("\n‚úÖ Todos los helpers est√°n disponibles")
```

---

## üìã Paso 3.5: Entender el Metadata de Celdas (Importante para Exportaci√≥n)

### ¬øQu√© es el Metadata de Celdas?

El **metadata de celdas** es informaci√≥n adicional que se almacena en cada celda de un notebook Jupyter. Esta informaci√≥n le dice a DSDL:

1. **Qu√© funciones exportar**: DSDL escanea el metadata para identificar qu√© celdas contienen funciones que deben exportarse al archivo `.py`
2. **C√≥mo identificar las funciones**: El campo `"name"` en el metadata indica el tipo de funci√≥n (ej: `"mltkc_init"`, `"mltkc_fit"`, etc.)
3. **Protecci√≥n de celdas**: El campo `"deletable": false` previene que se eliminen accidentalmente celdas cr√≠ticas

### ¬øPor qu√© es Importante?

**Sin el metadata correcto**, DSDL **NO exportar√°** las funciones al archivo `.py`, y cuando intentes usar el modelo desde Splunk, obtendr√°s errores como:

```
AttributeError: module 'app.model.mi_modelo' has no attribute 'fit'
```

**Con el metadata correcto**, DSDL autom√°ticamente:
- ‚úÖ Identifica las funciones requeridas (`init`, `fit`, `apply`, `summary`, `save`, `load`)
- ‚úÖ Las exporta al archivo `.py` cuando guardas el notebook
- ‚úÖ Permite que Splunk las llame desde ML-SPL

### ¬øC√≥mo Funciona el Proceso de Exportaci√≥n?

```
1. DS escribe c√≥digo en celda: def fit(model, df, param): ...
2. DS agrega metadata a la celda: {"name": "mltkc_fit", ...}
3. DS guarda el notebook (.ipynb)
   ‚Üì
4. DSDL escanea el notebook
5. DSDL encuentra celdas con metadata "mltkc_*"
6. DSDL extrae el c√≥digo de esas celdas
7. DSDL crea/actualiza: /srv/app/model/mi_modelo.py
   ‚Üì
8. Splunk ejecuta: | fit MLTKContainer algo=mi_modelo ...
9. DSDL importa mi_modelo.py y ejecuta fit()
```

### Metadata Completo para Cada Funci√≥n

A continuaci√≥n, el metadata completo que debes agregar a cada celda de c√≥digo que contiene una funci√≥n requerida:

#### 1. Metadata para `init()` - Funci√≥n Requerida

```json
{
    "deletable": false,
    "name": "mltkc_init",
    "trusted": true,
    "editable": true,
    "slideshow": {
        "slide_type": ""
    },
    "tags": []
}
```

**D√≥nde agregarlo**: En la celda de c√≥digo que contiene `def init(df, param):`

#### 2. Metadata para `fit()` - Funci√≥n Requerida

```json
{
    "deletable": false,
    "name": "mltkc_fit",
    "trusted": true,
    "editable": true,
    "slideshow": {
        "slide_type": ""
    },
    "tags": []
}
```

**D√≥nde agregarlo**: En la celda de c√≥digo que contiene `def fit(model, df, param):`

#### 3. Metadata para `apply()` - Funci√≥n Requerida

```json
{
    "deletable": false,
    "name": "mltkc_apply",
    "trusted": true,
    "editable": true,
    "slideshow": {
        "slide_type": ""
    },
    "tags": []
}
```

**D√≥nde agregarlo**: En la celda de c√≥digo que contiene `def apply(model, df, param):`

#### 4. Metadata para `summary()` - Funci√≥n Requerida

```json
{
    "deletable": false,
    "name": "mltkc_summary",
    "trusted": true,
    "editable": true,
    "slideshow": {
        "slide_type": ""
    },
    "tags": []
}
```

**D√≥nde agregarlo**: En la celda de c√≥digo que contiene `def summary(model=None):`

#### 5. Metadata para `save()` - Funci√≥n Requerida

```json
{
    "editable": true,
    "slideshow": {
        "slide_type": ""
    },
    "tags": [],
    "deletable": false,
    "name": "mltkc_save"
}
```

**D√≥nde agregarlo**: En la celda de c√≥digo que contiene `def save(model, name):`

**‚ö†Ô∏è IMPORTANTE**: Esta funci√≥n es llamada autom√°ticamente por DSDL despu√©s de `fit()`. Aseg√∫rate de que:
- La firma sea: `def save(model, name):`
- Retorne el modelo: `return model`

#### 6. Metadata para `load()` - Funci√≥n Opcional

```json
{
    "editable": true,
    "slideshow": {
        "slide_type": ""
    },
    "tags": [],
    "deletable": false,
    "name": "mltkc_load"
}
```

**D√≥nde agregarlo**: En la celda de c√≥digo que contiene `def load(name):`

**‚ö†Ô∏è NOTA**: Esta funci√≥n es opcional. DSDL NO la llama autom√°ticamente, pero es √∫til para desarrollo local.

### ¬øQu√© Hace Cada Campo del Metadata?

| Campo | Valor | Prop√≥sito |
|-------|-------|-----------|
| `"name"` | `"mltkc_init"`, `"mltkc_fit"`, etc. | **CR√çTICO**: Identifica la funci√≥n para DSDL. Sin esto, DSDL no exportar√° la funci√≥n. |
| `"deletable"` | `false` | Previene que se elimine accidentalmente la celda (importante para funciones requeridas) |
| `"trusted"` | `true` | Indica que la celda es confiable y puede ejecutarse sin restricciones de seguridad |
| `"editable"` | `true` | Permite editar la celda (siempre `true` para desarrollo) |
| `"slideshow"` | `{"slide_type": ""}` | Configuraci√≥n para presentaciones (no relevante para DSDL, pero parte del formato est√°ndar) |
| `"tags"` | `[]` | Etiquetas opcionales para organizar celdas (vac√≠o por defecto) |

### C√≥mo Agregar el Metadata en JupyterLab (M√©todo Visual)

1. **Selecciona la celda de c√≥digo** que contiene la funci√≥n (ej: `def init(df, param):`)
   - ‚ö†Ô∏è **IMPORTANTE**: Debe ser la celda de C√ìDIGO, NO la celda markdown (ej: "### Stage 1 - init")

2. **Activa el editor de metadata**:
   - Ve a: **View ‚Üí Cell Toolbar ‚Üí Edit Metadata**
   - O haz clic derecho en la celda ‚Üí **Edit Metadata**

3. **Copia y pega el metadata completo**:
   - Se abrir√° un editor JSON en la parte inferior de la celda
   - Borra cualquier contenido existente
   - Pega el metadata correspondiente (ej: para `init()`, usa el metadata de `mltkc_init`)

4. **Aplica los cambios**:
   - Haz clic en **"Apply"** o **"Save"**
   - El metadata se guardar√° en la celda

5. **Guarda el notebook**:
   - Presiona **Cmd+S** (Mac) o **Ctrl+S** (Windows/Linux)
   - DSDL autom√°ticamente exportar√° las funciones al archivo `.py`

### Verificaci√≥n del Metadata

Despu√©s de agregar el metadata, verifica que:

1. **El metadata est√° en la celda correcta**:
   - ‚úÖ Celda de c√≥digo con `def init(...):` ‚Üí metadata `"name": "mltkc_init"`
   - ‚ùå Celda markdown "### Stage 1 - init" ‚Üí NO debe tener este metadata

2. **El archivo `.py` exportado contiene las funciones**:
   ```python
   # En JupyterLab, ejecuta:
   import os
   py_file = f"/srv/app/model/{MODEL_NAME}.py"
   if os.path.exists(py_file):
       with open(py_file, 'r') as f:
           content = f.read()
           required_functions = ['def init(', 'def fit(', 'def apply(', 'def summary(', 'def save(']
           for func in required_functions:
               if func in content:
                   print(f"‚úÖ {func} encontrada")
               else:
                   print(f"‚ùå {func} NO encontrada")
   ```

3. **No hay celdas markdown vac√≠as con metadata**:
   - Si encuentras celdas markdown con metadata `"name": "mltkc_*"`, puedes eliminarlas o dejarlas (no afectan)

### Errores Comunes y Soluciones

| Error | Causa | Soluci√≥n |
|-------|-------|----------|
| `AttributeError: module has no attribute 'fit'` | El metadata no est√° en la celda de c√≥digo | Mueve el metadata a la celda de c√≥digo que contiene `def fit()` |
| El archivo `.py` no tiene las funciones | El metadata est√° mal formado o falta el campo `"name"` | Verifica que el JSON sea v√°lido y tenga `"name": "mltkc_*"` |
| No puedo borrar una celda | Tiene `"deletable": false` | Esto es intencional. Si necesitas borrarla, primero quita el metadata |
| Funciones duplicadas en `.py` | Hay m√∫ltiples celdas con el mismo metadata | Aseg√∫rate de tener solo UNA celda con cada `"name": "mltkc_*"` |

### Orden de las Funciones en el Notebook

El orden recomendado de las funciones en el notebook es:

1. `init()` - Inicializaci√≥n del modelo
2. `fit()` - Entrenamiento del modelo
3. `apply()` - Inferencia/predicci√≥n
4. `save()` - Guardado del modelo (requerido)
5. `load()` - Carga del modelo (opcional)
6. `summary()` - Metadatos del modelo

**Nota**: DSDL no requiere un orden espec√≠fico, pero este orden facilita la lectura y el mantenimiento.

---

## üé® Paso 4: Definir Funci√≥n `init()` - Inicializaci√≥n del Modelo

### 4.1 Crear Funci√≥n `init()` con Arquitectura de Autoencoder

En la c√©lula marcada con `# mltkc_init`, reemplaza con:

```python
# mltkc_init
# initialize the model
# params: data and parameters
# returns the model object which will be used as a reference to call fit, apply and summary subsequently

def init(df, param):
    """
    Inicializar autoencoder para detecci√≥n de anomal√≠as.
    
    Args:
        df: DataFrame con datos de Splunk
        param: Diccionario con par√°metros del modelo
    
    Returns:
        model: Modelo Keras compilado
    """
    print(f"üîß Inicializando modelo: {MODEL_NAME}")
    
    # Obtener features del DataFrame
    if 'feature_variables' in param:
        feature_cols = param['feature_variables']
    else:
        # Si no hay feature_variables definidas, usar todas las num√©ricas
        feature_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        if not feature_cols:
            # Fallback: buscar columnas que empiecen con 'feature_'
            feature_cols = [col for col in df.columns if col.startswith('feature_')]
    
    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])
    
    print(f"üìä Shape de los datos: {X.shape}")
    print(f"üìã Features seleccionadas: {len(X.columns)}")
    
    input_dim = X.shape[1]
    
    # Par√°metros del modelo (con valores por defecto)
    encoding_dim = 10  # Dimensi√≥n de la capa oculta (bottleneck)
    if 'options' in param and 'params' in param['options']:
        if 'encoding_dim' in param['options']['params']:
            encoding_dim = int(param['options']['params']['encoding_dim'])
        if 'components' in param['options']['params']:
            encoding_dim = int(param['options']['params']['components'])
    
    activation = 'relu'
    if 'options' in param and 'params' in param['options']:
        if 'activation' in param['options']['params']:
            activation = param['options']['params']['activation']
    
    print(f"‚öôÔ∏è  Par√°metros del modelo:")
    print(f"   - Input dimension: {input_dim}")
    print(f"   - Encoding dimension: {encoding_dim}")
    print(f"   - Activation: {activation}")
    
    # Construir autoencoder
    # Encoder
    encoder = keras.layers.Dense(
        encoding_dim, 
        activation=activation,
        input_shape=(input_dim,),
        name='encoder'
    )
    
    # Decoder
    decoder = keras.layers.Dense(
        input_dim,
        activation=activation,
        name='decoder'
    )
    
    # Modelo completo
    model = keras.Sequential([
        encoder,
        decoder
    ], name='Autoencoder')
    
    # Compilar modelo
    model.compile(
        optimizer='adam',
        loss='mse',  # Mean Squared Error para autoencoder
        metrics=['mae']  # Mean Absolute Error como m√©trica adicional
    )
    
    print(f"‚úÖ Modelo compilado exitosamente")
    print(f"üìê Arquitectura: {input_dim} ‚Üí {encoding_dim} ‚Üí {input_dim}")
    
    return model
```

### 4.2 Probar Funci√≥n `init()` Localmente

```python
# THIS CELL IS NOT EXPORTED - Test init localmente
# Crear datos dummy para probar
test_df = pd.DataFrame({
    'feature_0': np.random.randn(100),
    'feature_1': np.random.randn(100),
    'feature_2': np.random.randn(100),
    'feature_3': np.random.randn(100),
    'feature_4': np.random.randn(100)
})

test_param = {
    'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],
    'options': {
        'params': {
            'encoding_dim': 10
        }
    }
}

test_model = init(test_df, test_param)
print("\nüìä Resumen del modelo:")
test_model.summary()
```

**‚úÖ Validaci√≥n**: Verifica que el modelo se inicializa correctamente sin errores.

---

## üèãÔ∏è Paso 5: Definir Funci√≥n `fit()` - Entrenamiento con Telemetr√≠a

### 5.1 Crear Funci√≥n `fit()` con Helpers Empresariales

En la c√©lula marcada con `# mltkc_stage_create_model_fit`, reemplaza con:

```python
# mltkc_stage_create_model_fit
# returns a fit info json object

def fit(model, df, param):
    """
    Entrenar autoencoder con telemetr√≠a autom√°tica.
    
    Args:
        model: Modelo Keras inicializado
        df: DataFrame con datos de entrenamiento
        param: Diccionario con par√°metros de entrenamiento
    
    Returns:
        dict: Informaci√≥n del entrenamiento (historial, m√©tricas, etc.)
    """
    print(f"üöÄ Iniciando entrenamiento del modelo: {MODEL_NAME}")
    
    returns = {}
    
    # Obtener features
    if 'feature_variables' in param:
        feature_cols = param['feature_variables']
    else:
        feature_cols = [col for col in df.columns if col.startswith('feature_')]
        if not feature_cols:
            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])
    
    print(f"üìä Datos de entrenamiento: {X.shape[0]} muestras, {X.shape[1]} features")
    
    # Preprocesamiento: Normalizaci√≥n
    print("üîß Aplicando preprocesamiento (normalizaci√≥n)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
    
    # Guardar scaler en returns para uso posterior
    returns['scaler'] = scaler
    
    # Par√°metros de entrenamiento
    epochs = 50
    batch_size = 32
    validation_split = 0.2
    
    if 'options' in param and 'params' in param['options']:
        if 'epochs' in param['options']['params']:
            epochs = int(param['options']['params']['epochs'])
        if 'batch_size' in param['options']['params']:
            batch_size = int(param['options']['params']['batch_size'])
        if 'validation_split' in param['options']['params']:
            validation_split = float(param['options']['params']['validation_split'])
    
    print(f"‚öôÔ∏è  Par√°metros de entrenamiento:")
    print(f"   - Epochs: {epochs}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Validation split: {validation_split}")
    
    # Callback para TensorBoard (opcional)
    log_dir = f"/srv/notebooks/logs/fit/{MODEL_NAME}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"
    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=log_dir,
        histogram_freq=1
    )
    
    # Callback personalizado para logging de telemetr√≠a
    class TelemetryCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            """Enviar m√©tricas de cada √©poca a Splunk"""
            logs = logs or {}
            try:
                # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy/Pandas a tipos nativos de Python para JSON serialization
                # Los valores int64/float64 de NumPy no son serializables a JSON directamente
                epoch_value = int(epoch + 1)  # Convertir a int nativo
                loss_value = float(logs.get('loss', 0)) if logs.get('loss') is not None else 0.0
                val_loss_value = float(logs.get('val_loss', 0)) if logs.get('val_loss') is not None else 0.0
                mae_value = float(logs.get('mae', 0)) if logs.get('mae') is not None else 0.0
                val_mae_value = float(logs.get('val_mae', 0)) if logs.get('val_mae') is not None else 0.0
                
                log_training_step(
                    model_name=MODEL_NAME,
                    epoch=epoch_value,
                    loss=loss_value,
                    val_loss=val_loss_value,
                    mae=mae_value,
                    val_mae=val_mae_value
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Error enviando telemetr√≠a en √©poca {epoch + 1}: {e}")
                import traceback
                print(f"   Traceback completo: {traceback.format_exc()}")
    
    telemetry_callback = TelemetryCallback()
    
    # Entrenar modelo
    print("\nüèãÔ∏è  Iniciando entrenamiento...")
    history = model.fit(
        x=X_scaled_df,
        y=X_scaled_df,  # Autoencoder: input = output
        epochs=epochs,
        batch_size=batch_size,
        validation_split=validation_split,
        verbose=1,
        callbacks=[tensorboard_callback, telemetry_callback]
    )
    
    returns['fit_history'] = history
    returns['model_epochs'] = epochs
    returns['model_batch_size'] = batch_size
    returns['scaler'] = scaler  # Guardar scaler para uso en apply
    
    # Evaluar modelo en datos completos
    print("\nüìä Evaluando modelo en datos completos...")
    test_results = model.evaluate(X_scaled_df, X_scaled_df, verbose=0)
    returns['model_loss'] = test_results[0]
    returns['model_mae'] = test_results[1] if len(test_results) > 1 else None
    
    print(f"‚úÖ Entrenamiento completado")
    print(f"   - Loss final: {test_results[0]:.6f}")
    if len(test_results) > 1:
        print(f"   - MAE final: {test_results[1]:.6f}")
    
    # Calcular m√©tricas de reconstrucci√≥n
    print("\nüìà Calculando m√©tricas de reconstrucci√≥n...")
    X_pred = model.predict(X_scaled_df, verbose=0)
    
    # Calcular MSE y RMSE
    mse = mean_squared_error(X_scaled_df.values, X_pred)
    rmse = np.sqrt(mse)
    
    returns['mse'] = float(mse)
    returns['rmse'] = float(rmse)
    
    print(f"   - MSE: {mse:.6f}")
    print(f"   - RMSE: {rmse:.6f}")
    
    # Enviar m√©tricas finales a Splunk (telemetr√≠a)
    try:
        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy/Pandas a tipos nativos de Python para JSON serialization
        # Los valores int64/float64 de NumPy no son serializables a JSON directamente
        mae_value = float(returns['model_mae']) if returns['model_mae'] is not None else None
        rmse_value = float(rmse) if rmse is not None else None
        mse_value = float(mse) if mse is not None else None
        loss_value = float(test_results[0]) if test_results[0] is not None else None
        
        log_metrics(
            model_name=MODEL_NAME,
            r2_score=None,  # Autoencoder no tiene R¬≤ tradicional
            mae=mae_value,
            rmse=rmse_value,
            mse=mse_value,
            loss=loss_value,
            app_name=APP_NAME,
            model_version=VERSION,
            project=USE_CASE
        )
        print("‚úÖ M√©tricas enviadas a Splunk")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error enviando m√©tricas a Splunk: {e}")
        import traceback
        print(f"   Traceback completo: {traceback.format_exc()}")
    
    return returns
```

### 5.2 Probar Funci√≥n `fit()` Localmente

```python
# THIS CELL IS NOT EXPORTED - Test fit localmente
# Usar datos dummy m√°s grandes
test_df_fit = pd.DataFrame({
    'feature_0': np.random.randn(500),
    'feature_1': np.random.randn(500),
    'feature_2': np.random.randn(500),
    'feature_3': np.random.randn(500),
    'feature_4': np.random.randn(500)
})

test_param_fit = {
    'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],
    'options': {
        'params': {
            'epochs': '10',  # Pocas √©pocas para prueba r√°pida
            'batch_size': '32',
            'validation_split': '0.2'
        }
    }
}

# Crear modelo de prueba
test_model_fit = init(test_df_fit, test_param_fit)

# Entrenar (esto puede tomar unos minutos)
print("‚è≥ Entrenando modelo de prueba (esto tomar√° unos minutos)...")
fit_results = fit(test_model_fit, test_df_fit, test_param_fit)

print("\n‚úÖ Test de fit completado exitosamente")
print(f"   - Loss: {fit_results.get('model_loss', 'N/A')}")
print(f"   - MSE: {fit_results.get('mse', 'N/A')}")
```

**‚úÖ Validaci√≥n**: Verifica que el entrenamiento se ejecuta sin errores y que las m√©tricas se calculan correctamente.

---

## üîÆ Paso 6: Definir Funci√≥n `apply()` - Inferencia con Detecci√≥n de Anomal√≠as

### 6.1 Crear Funci√≥n `apply()` con C√°lculo de Anomal√≠as

En la c√©lula marcada con `# mltkc_stage_create_model_apply`, reemplaza con:

```python
# mltkc_stage_create_model_apply

def apply(model, df, param):
    """
    Aplicar autoencoder para detecci√≥n de anomal√≠as.
    
    Args:
        model: Modelo Keras entrenado
        df: DataFrame con datos nuevos para inferencia
        param: Diccionario con par√°metros (debe contener scaler de fit)
    
    Returns:
        DataFrame: DataFrame con reconstrucciones y scores de anomal√≠a
    """
    print(f"üîÆ Aplicando modelo: {MODEL_NAME}")
    
    # Obtener features (debe coincidir con las usadas en fit)
    if 'feature_variables' in param:
        feature_cols = param['feature_variables']
    else:
        feature_cols = [col for col in df.columns if col.startswith('feature_')]
        if not feature_cols:
            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    X = df[feature_cols] if feature_cols else df.select_dtypes(include=[np.number])
    
    print(f"üìä Datos de inferencia: {X.shape[0]} muestras, {X.shape[1]} features")
    
    # Obtener scaler del entrenamiento (desde param o fit_results)
    scaler = None
    if 'scaler' in param:
        scaler = param['scaler']
    elif hasattr(model, 'scaler'):
        scaler = model.scaler
    
    # Aplicar normalizaci√≥n
    if scaler is not None:
        # Usar scaler del entrenamiento
        X_scaled = scaler.transform(X)
        print("‚úÖ Usando scaler del entrenamiento")
    else:
        # Crear nuevo scaler si no est√° disponible (fallback)
        print("‚ö†Ô∏è  Scaler no encontrado en param. Aplicando normalizaci√≥n nueva...")
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
    
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
    
    # Predecir reconstrucciones
    print("üîÑ Calculando reconstrucciones...")
    X_reconstructed = model.predict(X_scaled_df, verbose=0)
    X_reconstructed_df = pd.DataFrame(X_reconstructed, columns=X.columns, index=X.index)
    
    # Calcular error de reconstrucci√≥n (MSE por muestra)
    reconstruction_error = np.mean((X_scaled_df.values - X_reconstructed_df.values) ** 2, axis=1)
    
    # Calcular threshold para anomal√≠as (percentil 95)
    # En producci√≥n, este threshold deber√≠a venir del conjunto de entrenamiento
    anomaly_threshold = np.percentile(reconstruction_error, 95)
    
    # Detectar anomal√≠as
    is_anomaly = reconstruction_error > anomaly_threshold
    anomaly_score = reconstruction_error / (anomaly_threshold + 1e-10)  # Normalizar score
    
    print(f"üìä Estad√≠sticas de reconstrucci√≥n:")
    print(f"   - Error medio: {np.mean(reconstruction_error):.6f}")
    print(f"   - Error mediano: {np.median(reconstruction_error):.6f}")
    print(f"   - Threshold (percentil 95): {anomaly_threshold:.6f}")
    print(f"   - Anomal√≠as detectadas: {np.sum(is_anomaly)} / {len(is_anomaly)} ({100*np.mean(is_anomaly):.2f}%)")
    
    # Construir DataFrame de resultados
    results = pd.DataFrame({
        'reconstruction_error': reconstruction_error,
        'anomaly_score': anomaly_score,
        'is_anomaly': is_anomaly.astype(int)
    }, index=X.index)
    
    # Agregar reconstrucciones como columnas
    for i, col in enumerate(X.columns):
        results[f'reconstruction_{col}'] = X_reconstructed_df[col].values
        results[f'original_{col}'] = X[col].values
    
    print(f"‚úÖ Inferencia completada")
    print(f"   - Shape de resultados: {results.shape}")
    
    # Enviar telemetr√≠a de inferencia a Splunk
    try:
        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy/Pandas a tipos nativos de Python para JSON serialization
        # Los valores int64/float64 de NumPy no son serializables a JSON directamente
        
        # IMPORTANTE: Usar .item() para convertir scalars NumPy a tipos nativos de Python
        # Esto es m√°s robusto que int() o float() porque maneja todos los tipos NumPy
        num_predictions = int(len(df))  # len() ya retorna int nativo
        
        # Para valores NumPy, usar .item() si est√° disponible, sino usar int()/float()
        if hasattr(is_anomaly.sum(), 'item'):
            num_anomalies = int(is_anomaly.sum().item())
        else:
            num_anomalies = int(is_anomaly.sum())
        
        if hasattr(reconstruction_error.mean(), 'item'):
            avg_reconstruction_error = float(reconstruction_error.mean().item())
        else:
            avg_reconstruction_error = float(reconstruction_error.mean())
        
        if hasattr(anomaly_threshold, 'item'):
            anomaly_threshold_native = float(anomaly_threshold.item())
        else:
            anomaly_threshold_native = float(anomaly_threshold)
        
        # ‚ö†Ô∏è DIAGN√ìSTICO: Verificar que todos los valores son serializables a JSON
        # Esto ayuda a identificar problemas antes de pasarlos al helper
        import json
        telemetry_data = {
            "model_name": MODEL_NAME,
            "num_predictions": num_predictions,
            "num_anomalies": num_anomalies,
            "avg_reconstruction_error": avg_reconstruction_error,
            "anomaly_threshold": anomaly_threshold_native,
            "app_name": APP_NAME,
            "model_version": VERSION,
            "project": USE_CASE
        }
        
        # Eliminar valores None
        telemetry_data = {k: v for k, v in telemetry_data.items() if v is not None}
        
        # Verificar serializaci√≥n JSON ANTES de llamar al helper
        try:
            json.dumps(telemetry_data)
            print("‚úÖ Todos los valores son serializables a JSON")
        except TypeError as e:
            print(f"‚ùå ERROR DE SERIALIZACI√ìN ANTES DEL HELPER: {e}")
            print(f"   Valores problem√°ticos:")
            for k, v in telemetry_data.items():
                try:
                    json.dumps({k: v})
                except TypeError:
                    print(f"      - {k}: {type(v)} = {v}")
                    # Convertir cualquier valor NumPy restante
                    if hasattr(v, 'item'):
                        telemetry_data[k] = v.item()
                    elif isinstance(v, (np.integer, np.floating)):
                        telemetry_data[k] = float(v) if isinstance(v, np.floating) else int(v)
            
            # Intentar de nuevo
            try:
                json.dumps(telemetry_data)
                print("‚úÖ Valores corregidos, ahora son serializables")
            except TypeError as e2:
                print(f"‚ùå ERROR PERSISTENTE: {e2}")
                raise  # Re-lanzar el error para que se capture en el except externo
        
        # ‚ö†Ô∏è CR√çTICO: Importar funciones de telemetr√≠a DENTRO del bloque try-except
        # Esto asegura que las funciones est√©n disponibles cuando se llamen
        # Importar log_prediction si est√° disponible
        try:
            from telemetry_helper import log_prediction
            log_prediction(
                model_name=telemetry_data["model_name"],
                num_predictions=telemetry_data["num_predictions"],
                num_anomalies=telemetry_data["num_anomalies"],
                avg_reconstruction_error=telemetry_data["avg_reconstruction_error"],
                anomaly_threshold=telemetry_data["anomaly_threshold"],
                app_name=telemetry_data["app_name"],
                model_version=telemetry_data["model_version"],
                owner=OWNER if 'OWNER' in globals() else None,
                project=telemetry_data["project"]
            )
            print("‚úÖ Telemetr√≠a de inferencia enviada a Splunk")
        except ImportError:
            # Si log_prediction no existe, usar log_metrics como alternativa
            from telemetry_helper import log_metrics
            log_metrics(
                model_name=telemetry_data["model_name"],
                num_predictions=telemetry_data["num_predictions"],
                num_anomalies=telemetry_data["num_anomalies"],
                avg_reconstruction_error=telemetry_data["avg_reconstruction_error"],
                anomaly_threshold=telemetry_data["anomaly_threshold"],
                app_name=telemetry_data["app_name"],
                model_version=telemetry_data["model_version"],
                project=telemetry_data["project"]
            )
            print("‚úÖ Telemetr√≠a de inferencia enviada a Splunk (usando log_metrics)")
        except Exception as telemetry_error:
            # Capturar cualquier otro error de telemetr√≠a (no solo ImportError)
            print(f"‚ö†Ô∏è  Error en telemetr√≠a (despu√©s de verificaci√≥n JSON): {telemetry_error}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()}")
            # No re-lanzar el error para que apply() pueda continuar
    except Exception as e:
        print(f"‚ö†Ô∏è  Error enviando telemetr√≠a de inferencia a Splunk: {e}")
        import traceback
        print(f"   Traceback completo: {traceback.format_exc()}")
    
    return results
```

### 6.2 Probar Funci√≥n `apply()` Localmente

```python
# THIS CELL IS NOT EXPORTED - Test apply localmente
# Crear datos nuevos para inferencia
test_df_apply = pd.DataFrame({
    'feature_0': np.random.randn(100),
    'feature_1': np.random.randn(100),
    'feature_2': np.random.randn(100),
    'feature_3': np.random.randn(100),
    'feature_4': np.random.randn(100)
})

# Agregar scaler al param (simulando que viene de fit)
test_param_apply = {
    'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],
    'scaler': fit_results.get('scaler')  # Usar scaler del fit anterior
}

# Aplicar modelo
results = apply(test_model_fit, test_df_apply, test_param_apply)

print("\nüìä Primeras 10 filas de resultados:")
print(results.head(10))

print("\nüìà Estad√≠sticas de anomal√≠as:")
print(f"   - Total muestras: {len(results)}")
print(f"   - Anomal√≠as detectadas: {results['is_anomaly'].sum()}")
print(f"   - Porcentaje: {100 * results['is_anomaly'].mean():.2f}%")
```

**‚úÖ Validaci√≥n**: Verifica que la inferencia funciona y que detecta anomal√≠as correctamente.

---

## üìä Paso 7: Definir Funci√≥n `summary()` - Metadatos del Modelo

### 7.1 Crear Funci√≥n `summary()` Completa

En la c√©lula marcada con `# return model summary`, reemplaza con:

```python
# return model summary

def summary(model=None):
    """
    Proporcionar metadatos y resumen del modelo.
    
    Args:
        model: Modelo Keras (opcional)
    
    Returns:
        dict: Metadatos del modelo
    """
    returns = {
        "model_name": MODEL_NAME,
        "app_name": APP_NAME,
        "model_type": MODEL_TYPE,
        "use_case": USE_CASE,
        "version": VERSION,
        "version_info": {
            "tensorflow": tf.__version__,
            "keras": keras.__version__,
            "numpy": np.__version__,
            "pandas": pd.__version__
        }
    }
    
    if model is not None:
        # Guardar resumen del modelo como string
        s = []
        model.summary(print_fn=lambda x: s.append(x + '\n'))
        returns["model_summary"] = ''.join(s)
        
        # Informaci√≥n de la arquitectura
        # ‚ö†Ô∏è CR√çTICO: Convertir valores NumPy a tipos nativos de Python para JSON serialization
        # DSDL serializa el resultado de summary() a JSON, y valores NumPy causan errores
        total_params = model.count_params()
        trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])
        
        # Convertir a tipos nativos de Python
        if hasattr(total_params, 'item'):
            total_params = int(total_params.item())
        else:
            total_params = int(total_params)
        
        if hasattr(trainable_params, 'item'):
            trainable_params = int(trainable_params.item())
        else:
            trainable_params = int(trainable_params)
        
        returns["model_architecture"] = {
            "input_shape": str(model.input_shape) if hasattr(model, 'input_shape') else "N/A",
            "output_shape": str(model.output_shape) if hasattr(model, 'output_shape') else "N/A",
            "total_params": total_params,  # Ya convertido a int nativo
            "trainable_params": trainable_params  # Ya convertido a int nativo
        }
        
        # Informaci√≥n de capas
        returns["layers"] = []
        for i, layer in enumerate(model.layers):
            # Obtener output_shape de manera segura
            output_shape = "N/A"
            try:
                # En Keras 2.x/TensorFlow 2.x, intentar m√∫ltiples m√©todos
                if hasattr(layer, 'output') and layer.output is not None:
                    # M√©todo 1: Desde el tensor output (disponible despu√©s de build)
                    try:
                        output_shape = str(layer.output.shape)
                    except:
                        pass
                
                if output_shape == "N/A":
                    # M√©todo 2: Intentar obtener desde config
                    if hasattr(layer, 'get_config'):
                        config = layer.get_config()
                        if 'output_shape' in config:
                            output_shape = str(config['output_shape'])
                
                if output_shape == "N/A":
                    # M√©todo 3: Calcular si es posible
                    if callable(getattr(layer, 'compute_output_shape', None)):
                        # Necesitamos input_shape, intentar obtenerlo
                        if i == 0 and hasattr(model, 'input_shape') and model.input_shape:
                            # Primera capa: usar input_shape del modelo
                            computed = layer.compute_output_shape(model.input_shape)
                            output_shape = str(computed)
                        elif hasattr(layer, 'input_shape') and layer.input_shape:
                            # Capas intermedias: usar input_shape de la capa
                            computed = layer.compute_output_shape(layer.input_shape)
                            output_shape = str(computed)
            except Exception:
                # Si todo falla, usar "N/A"
                output_shape = "N/A"
            
            # Obtener par√°metros de manera segura
            params = 0
            try:
                params_raw = layer.count_params()
                # ‚ö†Ô∏è CR√çTICO: Convertir a tipo nativo de Python para JSON serialization
                if hasattr(params_raw, 'item'):
                    params = int(params_raw.item())
                else:
                    params = int(params_raw)
            except Exception:
                params = 0
            
            returns["layers"].append({
                "index": i,  # Ya es int nativo
                "name": layer.name,
                "type": type(layer).__name__,
                "output_shape": output_shape,
                "params": params  # Ya convertido a int nativo
            })
    
    return returns
```

### 7.2 Probar Funci√≥n `summary()`

```python
# THIS CELL IS NOT EXPORTED - Test summary
model_summary = summary(test_model_fit)
print("üìä Resumen del modelo:")
print(json.dumps(model_summary, indent=2, default=str))
```

**‚úÖ Validaci√≥n**: Verifica que `summary()` retorna toda la informaci√≥n necesaria.

---

## üíæ Paso 8: Definir Funci√≥n `save()` - REQUERIDA por DSDL

### ‚ö†Ô∏è IMPORTANTE: `save()` es REQUERIDA

**DSDL llama autom√°ticamente a `save()` despu√©s de ejecutar `fit()`**. Si esta funci√≥n no existe o tiene la firma incorrecta, ver√°s el error:
```
MLTKC error: /fit: ERROR: unable to save model. Ended with exception: module 'app.model.app1_autoencoder_demo_anomalias_v1' has no attribute 'save'
```

**Firma requerida**: `def save(model, name):`
- `model`: El modelo entrenado (retornado por `fit()`)
- `name`: Nombre del modelo (pasado por DSDL desde `into app:model_name`)

### 8.1 Crear Funci√≥n `save()` para Modelos Keras

**‚ö†Ô∏è IMPORTANTE**: La celda que contiene `save()` **DEBE tener metadata especial** para que DSDL la exporte autom√°ticamente.

**‚ö†Ô∏è ERROR COM√öN**: Si despu√©s de guardar el notebook el archivo `.py` NO tiene `save()`, verifica:
1. ‚úÖ La funci√≥n est√° definida como `def save(model, name):` (NO `def load()`)
2. ‚úÖ El comentario en la celda es `# mltkc_save` (NO `# mltkc_load`)
3. ‚úÖ **CR√çTICO**: El metadata `"name": "mltkc_save"` est√° en la **CELDA DE C√ìDIGO** que contiene `def save()`, NO en la celda markdown
4. ‚úÖ El metadata de la celda de c√≥digo tiene `"name": "mltkc_save"` (NO `"name": "mltkc_load"`)

**‚ö†Ô∏è ERROR M√ÅS COM√öN**: El metadata est√° en la celda markdown (ej: "### Stage 8 - save model") en lugar de estar en la celda de c√≥digo que contiene `def save()`. **El metadata DEBE estar en la celda de c√≥digo, NO en la celda markdown.**

**Pasos para crear la funci√≥n `save()`**:

1. **Crea una nueva celda de c√≥digo** despu√©s de `summary()`
2. **Configura el metadata de la celda** (en JupyterLab: View ‚Üí Cell Toolbar ‚Üí Edit Metadata)
   - Agrega: `"name": "mltkc_save"` en el metadata de la celda
3. **Agrega el c√≥digo de la funci√≥n** (‚ö†Ô∏è IMPORTANTE: debe ser `def save()`, NO `def load()`):

```python
# mltkc_save
# Funci√≥n REQUERIDA: DSDL llama a save(model, name) despu√©s de fit()

def save(model, name):
    """
    Guardar modelo Keras en disco.
    
    IMPORTANTE: Esta funci√≥n es llamada autom√°ticamente por DSDL despu√©s de fit().
    
    Args:
        model: Modelo Keras entrenado (retornado por fit())
        name: Nombre del modelo (pasado por DSDL desde "into app:model_name")
    
    Returns:
        model: Retorna el modelo (requerido por DSDL)
    """
    # Importar os si no est√° disponible (para cuando DSDL exporta el m√≥dulo)
    import os
    
    # Asegurar que el directorio existe
    os.makedirs(MODEL_DIRECTORY, exist_ok=True)
    
    # Guardar modelo Keras
    filepath = MODEL_DIRECTORY + name + ".keras"
    model.save(filepath)
    
    print(f"‚úÖ Modelo guardado en: {filepath}")
    print(f"üìä Tama√±o del archivo: {os.path.getsize(filepath) / (1024*1024):.2f} MB")
    
    # NOTA: Si tienes un scaler u otros objetos, gu√°rdalos tambi√©n
    # Ejemplo: si el scaler est√° en el modelo o en globals
    # from sklearn.externals import joblib  # o import joblib
    # if hasattr(model, 'scaler'):
    #     joblib.dump(model.scaler, MODEL_DIRECTORY + name + "_scaler.pkl")
    
    # DSDL espera que retornes el modelo
    return model
```

### 8.2 Probar Funci√≥n `save()` Localmente

```python
# THIS CELL IS NOT EXPORTED - Test save localmente
print("üíæ Probando funci√≥n save()...")

# Verificar que las variables necesarias existen
if 'test_model_fit' not in globals():
    print("‚ö†Ô∏è  test_model_fit no est√° definido.")
    print("   Necesitas ejecutar primero el test de fit() (Paso 6.2)")
    print("   Para crear un modelo de prueba r√°pido, ejecuta:")
    print("""
    # Crear datos dummy
    test_df_fit = pd.DataFrame({
        'feature_0': np.random.randn(100),
        'feature_1': np.random.randn(100),
        'feature_2': np.random.randn(100),
        'feature_3': np.random.randn(100),
        'feature_4': np.random.randn(100)
    })
    test_param_fit = {
        'feature_variables': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4'],
        'options': {'params': {'epochs': '5', 'batch_size': '32'}}
    }
    test_model_fit = init(test_df_fit, test_param_fit)
    fit_results = fit(test_model_fit, test_df_fit, test_param_fit)
    """)
else:
    try:
        # Asegurar que MODEL_DIRECTORY est√° definido
        try:
            model_dir = MODEL_DIRECTORY
        except NameError:
            model_dir = "/srv/app/model/data/"
        
        # Guardar modelo de prueba usando la firma correcta
        saved_model = save(test_model_fit, name="test_autoencoder")
        print(f"‚úÖ Modelo guardado exitosamente")
        
        # Verificar que el archivo existe
        filepath = model_dir + "test_autoencoder.keras"
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath) / (1024 * 1024)
            print(f"üìä Tama√±o del archivo: {file_size:.2f} MB")
            print(f"‚úÖ Archivo creado correctamente: {filepath}")
        else:
            print(f"‚ö†Ô∏è  Archivo no encontrado: {filepath}")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
```

**‚úÖ Validaci√≥n**: Verifica que `save()` funciona correctamente con la firma `save(model, name)`.

**Nota**: Si obtienes el error `name 'test_model_fit' is not defined`, necesitas ejecutar primero el test de `fit()` (Paso 6.2) para crear el modelo de prueba.

### 8.3 Funci√≥n Opcional `load()` para Desarrollo Local

**NOTA**: `load()` NO es requerida por DSDL, pero es √∫til para desarrollo local. Si quieres que DSDL la exporte, agrega el metadata `"name": "mltkc_load"` a la celda.

**Para que DSDL exporte `load()`** (opcional):

‚ö†Ô∏è **IMPORTANTE**: Para una explicaci√≥n completa sobre qu√© es el metadata, por qu√© es importante y c√≥mo agregarlo correctamente, consulta la **Secci√≥n 3.5: Entender el Metadata de Celdas** en esta gu√≠a.

1. **Crea una nueva celda de c√≥digo** despu√©s de `save()`
2. **Configura el metadata de la celda** (en JupyterLab: View ‚Üí Cell Toolbar ‚Üí Edit Metadata)
   - Agrega el metadata completo (ver **Paso 3.5** para m√°s detalles):
   ```json
   {
     "editable": true,
     "slideshow": {
       "slide_type": ""
     },
     "tags": [],
     "deletable": false,
     "name": "mltkc_load"
   }
   ```
3. **Agrega el c√≥digo de la funci√≥n**:

```python
# mltkc_load
# Funci√≥n opcional para cargar modelo guardado durante desarrollo
# DSDL NO llama a esta funci√≥n autom√°ticamente

def load(name):
    """
    Cargar modelo Keras desde disco.
    
    √ötil para desarrollo local o pruebas.
    DSDL NO usa esta funci√≥n autom√°ticamente.
    
    Args:
        name: Nombre del archivo (sin extensi√≥n)
    
    Returns:
        Model: Modelo Keras cargado
    """
    # Importar os si no est√° disponible
    import os
    
    # Asegurar que MODEL_DIRECTORY est√° definido (usar variable global o local)
    try:
        # Intentar usar MODEL_DIRECTORY global
        model_dir = MODEL_DIRECTORY
    except NameError:
        # Si no existe, usar valor por defecto
        model_dir = "/srv/app/model/data/"
    
    filepath = model_dir + name + ".keras"
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"‚ùå Archivo no encontrado: {filepath}")
    
    print(f"üì• Cargando modelo desde: {filepath}")
    model = keras.models.load_model(filepath)
    
    print(f"‚úÖ Modelo cargado exitosamente")
    print(f"üìä Arquitectura: {model.input_shape} ‚Üí {model.output_shape}")
    
    return model
```

### 8.4 Probar Funci√≥n `load()` Localmente (Opcional)

```python
# THIS CELL IS NOT EXPORTED - Test load localmente (opcional)
print("üì• Probando funci√≥n load()...")

# Verificar que el archivo existe antes de intentar cargarlo
import os

# Asegurar que MODEL_DIRECTORY est√° definido
try:
    model_dir = MODEL_DIRECTORY
except NameError:
    model_dir = "/srv/app/model/data/"

test_filepath = model_dir + "test_autoencoder.keras"
if not os.path.exists(test_filepath):
    print(f"‚ö†Ô∏è  Archivo no encontrado: {test_filepath}")
    print("   Necesitas ejecutar primero el test de save() (Paso 8.2)")
    print("   O aseg√∫rate de que test_model_fit existe y ejecuta:")
    print("   saved_model = save(test_model_fit, name='test_autoencoder')")
else:
    try:
        loaded_model = load("test_autoencoder")
        print("‚úÖ Modelo cargado exitosamente")
        
        # Verificar que son equivalentes (solo si test_model_fit existe)
        if 'test_model_fit' in globals():
            print("\nüîç Verificando que el modelo cargado funciona...")
            test_input = np.random.randn(1, 5)  # 5 features
            output_original = test_model_fit.predict(test_input, verbose=0)
            output_loaded = loaded_model.predict(test_input, verbose=0)
            
            if np.allclose(output_original, output_loaded):
                print("‚úÖ Los modelos producen resultados id√©nticos")
            else:
                print("‚ö†Ô∏è  Los modelos producen resultados diferentes")
        else:
            print("‚ö†Ô∏è  test_model_fit no est√° definido, no se puede verificar equivalencia")
            print("   Pero el modelo se carg√≥ correctamente ‚úÖ")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
```

**Nota**: Este test requiere que:
1. Se haya ejecutado el test de `save()` primero (Paso 8.2)
2. El archivo `test_autoencoder.keras` exista en `MODEL_DIRECTORY`

### 8.5 Notas sobre `save()` y DSDL

**¬øC√≥mo DSDL usa `save()`?**

Cuando ejecutas desde SPL:
```spl
| fit MLTKContainer algo=app1_autoencoder_demo_anomalias_v1 into app:demo_model_v1
```

DSDL autom√°ticamente:
1. Ejecuta `fit(model, df, param)` y obtiene el modelo entrenado
2. **Llama a `save(model, "demo_model_v1")`** ‚Üê REQUERIDO
3. Persiste el modelo guardado
4. Cuando ejecutas `apply demo_model_v1`, DSDL carga el modelo y lo pasa a `apply()`

**Importante**: 
- La funci√≥n `save()` **DEBE** tener la firma: `def save(model, name):`
- **DEBE** retornar el modelo: `return model`
- El par√°metro `name` viene de `into app:model_name` en SPL

---

## üß™ Paso 9: Testing End-to-End Local

### 9.1 Test Completo con Datos Reales de Splunk

```python
# THIS CELL IS NOT EXPORTED - Test E2E completo local
from dsdlsupport import SplunkSearch

print("=" * 60)
print("TEST END-TO-END COMPLETO")
print("=" * 60)

# 1. Obtener datos reales de Splunk
print("\nüì• Paso 1: Obteniendo datos de Splunk...")
search = SplunkSearch.SplunkSearch(
    search='index=demo_anomalias_data | head 500 | table feature_*'
)
df_real = search.as_df()
print(f"‚úÖ Datos obtenidos: {df_real.shape}")

# 2. Inicializar modelo
print("\nüîß Paso 2: Inicializando modelo...")
param_init = {
    'feature_variables': [col for col in df_real.columns if col.startswith('feature_')],
    'options': {
        'params': {
            'encoding_dim': 10
        }
    }
}
model_real = init(df_real, param_init)
print("‚úÖ Modelo inicializado")

# 3. Entrenar modelo
print("\nüèãÔ∏è  Paso 3: Entrenando modelo (esto tomar√° varios minutos)...")
param_fit = {
    'feature_variables': [col for col in df_real.columns if col.startswith('feature_')],
    'options': {
        'params': {
            'epochs': '20',  # Pocas √©pocas para test r√°pido
            'batch_size': '32',
            'validation_split': '0.2'
        }
    }
}
fit_results_real = fit(model_real, df_real, param_fit)
print("‚úÖ Modelo entrenado")

# 4. Aplicar modelo
print("\nüîÆ Paso 4: Aplicando modelo...")
param_apply = {
    'feature_variables': [col for col in df_real.columns if col.startswith('feature_')],
    'scaler': fit_results_real.get('scaler')
}
results_real = apply(model_real, df_real, param_apply)
print("‚úÖ Modelo aplicado")

# 5. Resumen
print("\nüìä Paso 5: Resumen del modelo...")
summary_real = summary(model_real)
print(f"‚úÖ Resumen generado")

# 6. Mostrar resultados
print("\n" + "=" * 60)
print("RESULTADOS FINALES")
print("=" * 60)
print(f"‚úÖ Modelo: {MODEL_NAME}")
print(f"‚úÖ Datos procesados: {df_real.shape[0]} muestras")
print(f"‚úÖ Anomal√≠as detectadas: {results_real['is_anomaly'].sum()} ({100*results_real['is_anomaly'].mean():.2f}%)")
print(f"‚úÖ Loss final: {fit_results_real.get('model_loss', 'N/A')}")
print(f"‚úÖ RMSE: {fit_results_real.get('rmse', 'N/A')}")

print("\nüéâ Test E2E completado exitosamente!")
```

**‚úÖ Validaci√≥n**: Verifica que todo el flujo funciona de extremo a extremo con datos reales.

---

## üíæ Paso 10: Guardar el Notebook - Publicaci√≥n Autom√°tica

### 10.1 Preparar Notebook para Guardado

**IMPORTANTE**: Antes de guardar, verifica:

1. **Todas las funciones est√°n definidas**:
   - [ ] `init(df, param)` con comentario `# mltkc_init` y metadata `"name": "mltkc_init"`
   - [ ] `fit(model, df, param)` con comentario `# mltkc_stage_create_model_fit` y metadata `"name": "mltkc_stage_create_model_fit"`
   - [ ] `apply(model, df, param)` con comentario `# mltkc_stage_create_model_apply` y metadata `"name": "mltkc_stage_create_model_apply"`
   - [ ] `summary(model=None)` con comentario adecuado
   - [ ] `save(model, name)` **REQUERIDA** - Con metadata `"name": "mltkc_save"` en la celda

2. **Imports est√°n en la c√©lula correcta**:
   - [ ] Imports principales en c√©lula con `# mltkc_import`
   - [ ] Helpers empresariales importados

3. **Sin errores de sintaxis**:
   - [ ] Ejecuta "Cell ‚Üí Run All" para verificar que no hay errores
   - [ ] Revisa que todas las c√©lulas ejecutan correctamente

### 10.2 Guardar el Notebook

1. En JupyterLab, haz clic en: **File ‚Üí Save Notebook**
2. O usa el atajo: `Cmd+S` (Mac) / `Ctrl+S` (Windows/Linux)

**Esto dispara autom√°ticamente**:
- DSDL escanea el notebook
- Extrae las funciones `init`, `fit`, `apply`, `summary`, `save`
- Exporta a `/srv/app/model/app1_autoencoder_demo_anomalias_v1.py`
- Publica el modelo para uso desde SPL

### 10.3 Verificar Exportaci√≥n Autom√°tica

```python
# THIS CELL IS NOT EXPORTED - Verificar exportaci√≥n
import os

model_file = f"/srv/app/model/{MODEL_NAME}.py"
print(f"üîç Verificando archivo exportado: {model_file}")

if os.path.exists(model_file):
    print(f"‚úÖ Archivo exportado encontrado!")
    print(f"üìÑ Tama√±o: {os.path.getsize(model_file)} bytes")
    
    # Leer primeras l√≠neas
    with open(model_file, 'r') as f:
        lines = f.readlines()[:20]
        print("\nüìù Primeras 20 l√≠neas del archivo exportado:")
        print("=" * 60)
        for i, line in enumerate(lines, 1):
            print(f"{i:3d}: {line.rstrip()}")
else:
    print(f"‚ö†Ô∏è  Archivo no encontrado a√∫n. Esto es normal si:")
    print(f"   - Acabas de guardar (puede tardar unos segundos)")
    print(f"   - Hay errores en las funciones")
    print(f"   - Las funciones no tienen los comentarios correctos")
```

### 10.4 Verificar Logs de DSDL

Si el archivo no aparece, revisa los logs:

```python
# THIS CELL IS NOT EXPORTED - Revisar logs
import subprocess

print("üîç Buscando logs de exportaci√≥n...")
# Nota: Los logs est√°n en el contenedor, no en el notebook
# Para ver logs, ejecuta en terminal del contenedor o revisa Splunk
print("üí° Para ver logs completos, ejecuta en terminal del contenedor:")
print("   docker logs <container-id> | grep -i export")
print("\nüí° O busca en Splunk:")
print("   index=_internal \"mltk-container\" export")
```

---

## üöÄ Paso 11: Validar Modelo desde Splunk SPL

### 11.1 Usar Modelo con `fit` desde SPL

Una vez guardado el notebook y exportado, prueba desde Splunk Web:

```spl
index=demo_anomalias_data
| head 1000
| fit MLTKContainer algo=app1_autoencoder_demo_anomalias_v1 epochs=20 batch_size=32 encoding_dim=10 from feature_* into app:demo_anomalias_model_v1
```

**Explicaci√≥n**:
- `algo=app1_autoencoder_demo_anomalias_v1`: Nombre del notebook (sin .ipynb)
- `epochs=20`: Par√°metro pasado a `fit()`
- `from feature_*`: Selecciona todas las columnas que empiezan con `feature_`
- `into app:demo_anomalias_model_v1`: Nombre del modelo guardado

### 11.2 Verificar que `fit` Funciona

Despu√©s de ejecutar el comando `fit`, deber√≠as ver:
- ‚úÖ Mensaje de √©xito
- ‚úÖ M√©tricas de entrenamiento
- ‚úÖ Modelo guardado

Si hay errores:
1. Revisa los logs en Splunk: `index=_internal "mltk-container" ERROR`
2. Verifica que el archivo `.py` se export√≥ correctamente
3. Verifica que las funciones tienen las firmas correctas

### 11.3 Usar Modelo con `apply` desde SPL

Una vez entrenado, prueba la inferencia:

```spl
index=demo_anomalias_data
| head 500
| apply demo_anomalias_model_v1
| table feature_*, reconstruction_error, anomaly_score, is_anomaly
| head 20
```

**Resultado esperado**: Deber√≠as ver columnas nuevas:
- `reconstruction_error`: Error de reconstrucci√≥n
- `anomaly_score`: Score normalizado de anomal√≠a
- `is_anomaly`: 1 si es anomal√≠a, 0 si no

### 11.4 Visualizar Anomal√≠as en Splunk

```spl
index=demo_anomalias_data
| apply demo_anomalias_model_v1
| stats count by is_anomaly
| eval anomaly_percentage = if(is_anomaly=1, count, 0) / sum(count) * 100
```

---

## üîß Troubleshooting

Si encuentras problemas durante el flujo E2E, consulta la **Gu√≠a de Troubleshooting** para soluciones detalladas:

üìÑ **`TROUBLESHOOTING.md`** - Gu√≠a completa de soluci√≥n de problemas

**Problemas comunes**:
- El notebook no se exporta autom√°ticamente
- Errores al ejecutar `fit` desde SPL
- Helpers no se importan
- Telemetr√≠a no llega a Splunk
- Error "no attribute 'save'"
- Problemas de serializaci√≥n JSON

**Para diagn√≥stico completo de telemetr√≠a**, consulta tambi√©n:
üìÑ **`DIAGNOSTICO_TELEMETRIA.md`** - Diagn√≥stico espec√≠fico de telemetr√≠a

---

## ‚úÖ Checklist Final de Publicaci√≥n

Antes de considerar el modelo "publicado y listo":

- [ ] Notebook guardado con nombre est√°ndar: `app1_autoencoder_demo_anomalias_v1.ipynb`
- [ ] Archivo `.py` exportado existe en `/srv/app/model/`
- [ ] Funci√≥n `init()` funciona correctamente
- [ ] Funci√≥n `fit()` entrena sin errores
- [ ] Funci√≥n `apply()` detecta anomal√≠as
- [ ] Funci√≥n `summary()` retorna metadatos
- [ ] Test E2E local funciona completamente
- [ ] Comando `fit` desde SPL funciona
- [ ] Comando `apply` desde SPL funciona
- [ ] Telemetr√≠a llega a Splunk (si est√° configurada)
- [ ] Anomal√≠as detectadas son razonables

---

## üìö Recursos Adicionales

### Documentaci√≥n
- **Documentaci√≥n DSDL**: https://docs.splunk.com/Documentation/DSDL
- **Gu√≠a de Troubleshooting**: `TROUBLESHOOTING.md` - Soluci√≥n de problemas comunes
- **Diagn√≥stico de Telemetr√≠a**: `DIAGNOSTICO_TELEMETRIA.md` - Diagn√≥stico espec√≠fico de telemetr√≠a

### Archivos del Sistema
- **Template empresarial**: `/dltk/notebooks_custom/template_empresa_base.ipynb`
- **Helpers empresariales**: `/dltk/notebooks_custom/helpers/`

### Splunk Queries
- **Logs de DSDL**: `index=_internal "mltk-container"`
- **M√©tricas del modelo**: `index=ml_metrics model_name=app1_autoencoder_demo_anomalias_v1`
- **Logs de entrenamiento**: `index=ml_model_logs model_name=app1_autoencoder_demo_anomalias_v1`

---

## üéØ Pr√≥ximos Pasos

Una vez que el modelo est√° publicado y funcionando:

1. **Monitorear m√©tricas**: Revisar `index=ml_metrics` regularmente
2. **Ajustar thresholds**: Modificar percentil de anomal√≠as seg√∫n necesidad
3. **Crear dashboards**: Visualizar anomal√≠as en tiempo real
4. **Configurar alertas**: Alertar cuando anomal√≠as superen umbral
5. **Refinar modelo**: Iterar con m√°s datos o arquitecturas diferentes

---

**¬°Felicidades! Has completado el workflow end-to-end de un Data Scientist con DSDL.** üéâ

